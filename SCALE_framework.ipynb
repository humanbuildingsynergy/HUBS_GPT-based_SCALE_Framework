{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a079fb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from textstat import flesch_reading_ease, flesch_kincaid_grade\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import openai\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d0f6a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69b3fd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMChatAnalyzer:\n",
    "    \"\"\"\n",
    "    Advanced analyzer for GPT conversations using LLM-based contextual understanding \n",
    "    with experimental context awareness.\n",
    "    \n",
    "    This class provides comprehensive analysis of user-GPT conversations including:\n",
    "    - True contextual concept detection using GPT-4o-mini\n",
    "    - Distinction between authentic user engagement and data-enhanced assistant analysis\n",
    "    - User engagement pattern analysis with experimental context consideration\n",
    "    - Conversation flow and coherence evaluation\n",
    "    - Energy domain-specific concept attribution with explanations\n",
    "    \n",
    "    Unlike embedding-based approaches, this analyzer uses language models to actually\n",
    "    understand the context and meaning of conversations, providing explainable results\n",
    "    while accounting for experimental data provision.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str = None, analysis_model: str = \"gpt-4o-mini\"):\n",
    "        \"\"\"\n",
    "        Initialize the analyzer with OpenAI's chat completion model for concept analysis.\n",
    "        \n",
    "        Args:\n",
    "            api_key (str): OpenAI API key. If None, will try to get from .env file or environment variable OPENAI_API_KEY\n",
    "            analysis_model (str): OpenAI chat model to use for analysis. Options:\n",
    "                                 - 'gpt-4o-mini': Fast, cost-effective, good performance (recommended)\n",
    "                                 - 'gpt-4o': Slower, more expensive, best performance\n",
    "                                 - 'gpt-4': High quality but more expensive\n",
    "                                 - 'gpt-3.5-turbo': Fastest and cheapest, decent performance\n",
    "        \n",
    "        The analyzer uses structured prompting with multi-factor confidence scoring to understand conversation content contextually,\n",
    "        while distinguishing between authentic user engagement and data-enhanced assistant analysis.\n",
    "        \"\"\"\n",
    "        # Load API key from .env file, parameter, or environment variable\n",
    "        if api_key:\n",
    "            openai.api_key = api_key\n",
    "        else:\n",
    "            # Try to get API key from .env file first, then environment variable\n",
    "            api_key = os.getenv('OPENAI_API_KEY')\n",
    "            if not api_key:\n",
    "                raise ValueError(\n",
    "                    \"OpenAI API key not found. Please either:\\n\"\n",
    "                    \"1. Create a .env file with OPENAI_API_KEY=your-key-here\\n\"\n",
    "                    \"2. Set OPENAI_API_KEY environment variable\\n\"\n",
    "                    \"3. Pass api_key parameter to the constructor\"\n",
    "                )\n",
    "            openai.api_key = api_key\n",
    "        \n",
    "        self.client = openai.OpenAI(api_key=api_key)\n",
    "        self.analysis_model = analysis_model\n",
    "        \n",
    "        print(f\"🧠 Initialized with OpenAI chat model: {analysis_model}\")\n",
    "        print(f\"🔑 API key loaded successfully from {'parameter' if api_key else '.env file/environment'}\")\n",
    "        print(f\"📊 Using experimental context-aware multi-factor confidence scoring framework\")\n",
    "        \n",
    "        # === EXPERIMENTAL CONTEXT-AWARE MULTI-FACTOR SCORING FRAMEWORK ===\n",
    "        self.scoring_factors = {\n",
    "            \"explicitness\": {\n",
    "                \"weight\": 0.30,\n",
    "                \"description\": \"How directly and clearly the concept is mentioned using appropriate terminology\",\n",
    "                \"user_criteria\": {\n",
    "                    1.0: \"Domain terminology used correctly in context (energy bills, HVAC, kWh)\",\n",
    "                    0.8: \"General energy terms used appropriately in context\",\n",
    "                    0.6: \"Related terms that imply concept understanding\", \n",
    "                    0.4: \"Indirect references requiring some inference\",\n",
    "                    0.2: \"Vague connection or unclear terminology\",\n",
    "                    0.0: \"No mention or connection\"\n",
    "                },\n",
    "                \"assistant_criteria\": {\n",
    "                    1.0: \"Precise domain terminology with technical accuracy (kWh, TOU rates, HVAC)\",\n",
    "                    0.8: \"Appropriate technical terms with clear explanations\",\n",
    "                    0.6: \"General energy terminology used correctly\",\n",
    "                    0.4: \"Basic energy terms with some accuracy\", \n",
    "                    0.2: \"Imprecise or unclear terminology\",\n",
    "                    0.0: \"No meaningful terminology used\"\n",
    "                }\n",
    "            },\n",
    "            \"depth\": {\n",
    "                \"weight\": 0.25,\n",
    "                \"description\": \"Quality and authenticity of engagement with the concept\",\n",
    "                \"user_criteria\": {\n",
    "                    1.0: \"Deep contextual engagement with detailed reasoning, constraints, or comprehensive understanding\",\n",
    "                    0.8: \"Substantial contextual engagement with clear reasoning or thoughtful analysis\",\n",
    "                    0.6: \"Moderate contextual engagement with some reasoning or understanding\",\n",
    "                    0.4: \"Basic contextual engagement with minimal reasoning or surface-level understanding\", \n",
    "                    0.2: \"Shallow engagement with little reasoning or very limited understanding\",\n",
    "                    0.0: \"No meaningful engagement demonstrated\"\n",
    "                },\n",
    "                \"assistant_criteria\": {\n",
    "                    1.0: \"Comprehensive multi-faceted analysis examining multiple dimensions of the concept\",\n",
    "                    0.8: \"Thorough analysis exploring several aspects or implications of the concept\",\n",
    "                    0.6: \"Moderate analysis covering key aspects with reasonable detail\",\n",
    "                    0.4: \"Basic analysis touching on main points with limited development\",\n",
    "                    0.2: \"Superficial analysis with minimal exploration or development\",\n",
    "                    0.0: \"No meaningful analytical processing demonstrated\"\n",
    "                }\n",
    "            },\n",
    "            \"consideration\": {\n",
    "                \"weight\": 0.25,\n",
    "                \"description\": \"Whether the concept was meaningfully present in the participant's thinking\",\n",
    "                \"user_criteria\": {\n",
    "                    1.0: \"Concept clearly influences user's decisions, preferences, or thinking\",\n",
    "                    0.8: \"Concept is actively considered in relation to user's situation\",\n",
    "                    0.6: \"Concept is acknowledged and shows contextual relevance\",\n",
    "                    0.4: \"Concept is mentioned with minimal contextual connection\",\n",
    "                    0.2: \"Concept is barely acknowledged or referenced\",\n",
    "                    0.0: \"Concept is not considered in user's thinking\"\n",
    "                },\n",
    "                \"assistant_criteria\": {\n",
    "                    1.0: \"Concept is fully integrated into analysis and recommendations\",\n",
    "                    0.8: \"Concept is clearly incorporated into response strategy\", \n",
    "                    0.6: \"Concept is meaningfully addressed in the analysis\",\n",
    "                    0.4: \"Concept is mentioned but not well integrated\",\n",
    "                    0.2: \"Concept is briefly touched upon\",\n",
    "                    0.0: \"Concept is not considered in the response\"\n",
    "                }\n",
    "            },\n",
    "            \"evidence\": {\n",
    "                \"weight\": 0.20,\n",
    "                \"description\": \"Quality and authenticity of supporting evidence provided\",\n",
    "                \"user_criteria\": {\n",
    "                    1.0: \"Contextual examples, specific constraints, or experiential details\",\n",
    "                    0.8: \"Clear situational context or constraints with details\",\n",
    "                    0.6: \"General contextual examples or reasonable situational context\",\n",
    "                    0.4: \"Some contextual information or basic examples\",\n",
    "                    0.2: \"Minimal supporting contextual information\",\n",
    "                    0.0: \"No contextual evidence or examples provided\"\n",
    "                },\n",
    "                \"assistant_criteria\": {\n",
    "                    1.0: \"Multiple high-quality sources: quantitative data + domain expertise + specific examples\",\n",
    "                    0.8: \"Strong primary source with additional supporting information\",\n",
    "                    0.6: \"Solid single source with reasonable supporting details\",\n",
    "                    0.4: \"Basic source material with minimal additional support\",\n",
    "                    0.2: \"Weak or limited source material with little support\",\n",
    "                    0.0: \"No credible supporting information provided\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # === ENERGY-SPECIFIC CONCEPT DEFINITIONS ===\n",
    "        # These definitions guide the LLM in understanding what to look for\n",
    "        self.energy_concepts = {\n",
    "            \"energy_consumption\": {\n",
    "                \"description\": \"Discussion of circuit or appliance energy consumption data, or analysis of how much energy circuits or appliances in a household consume.\",\n",
    "                \"examples\": [\n",
    "                    \"HVAC unit used 5000 kWh per month\",\n",
    "                    \"The pool pump was consuming the most energy in July\",\n",
    "                    \"I want to analyze my energy usage of appliances\",\n",
    "                    \"What is the energy consumption of my refrigerator?\"\n",
    "                    \"Which appliances used the most energy?\"\n",
    "                ]\n",
    "            },\n",
    "            \"cost_awareness\": {\n",
    "                \"description\": \"Discussion of electricity bills, energy costs, utility rates, time-of-use pricing, cost savings, or financial impact of energy decisions.\",\n",
    "                \"examples\": [\n",
    "                    \"Your electricity bill was $150 last month\",\n",
    "                    \"Peak hours are more expensive\",\n",
    "                    \"I want to reduce my energy bills\",\n",
    "                    \"Please consider time-of-use rates in your calculations\",\n",
    "                    \"How much will this save on my electricity bill?\"\n",
    "                ]\n",
    "            },\n",
    "            \"behavioral_change\": {\n",
    "                \"description\": \"Discussion of adjusting usage patterns, adopting energy-saving behaviors, modifying habits, or actionable steps to reduce energy consumption.\",\n",
    "                \"examples\": [\n",
    "                    \"I suggest running the dishwasher at night\",\n",
    "                    \"Your HVAC running times can be adjusted to off-peak hours\",\n",
    "                    \"Let's evalaute how I can change my energy usage patterns to reduce energy bills\",\n",
    "                    \"I can't change my energy use patterns for the refrigerator\",\n",
    "                    \"I'm willing to adjust my thermostat settings up to 2 degree Fahrenheit\"\n",
    "                ]\n",
    "            },\n",
    "            \"use_flexibility\": {\n",
    "                \"description\": \"Discussion of how regularly or irregularly appliances are used, flexibility of operation, patterns of appliance operation throughout different time periods.\",\n",
    "                \"examples\": [\n",
    "                    \"Dishwasher is used irregularly throughout the day\",\n",
    "                    \"Let's see if I am using my microwave at regular times\"\n",
    "                ]\n",
    "            },\n",
    "            \"use_frequency\": {\n",
    "                \"description\": \"Discussion of how often appliances are used, frequency of operation.\",\n",
    "                \"examples\": [\n",
    "                    \"HVAC is running consistently during peak hours\",\n",
    "                    \"I run my dishwasher every day\",\n",
    "                    \"The pool pump operates 6 hours daily\",\n",
    "                    \"We rarely use the oven\",\n",
    "                ]\n",
    "            },\n",
    "            \"comfort_association\": {\n",
    "                \"description\": \"Discussion of comfort or convenience issues when appliances are used in different times.\",\n",
    "                \"examples\": [\n",
    "                    \"I don't want to change my thermostat settings too much\",\n",
    "                    \"I should not run the dishwasher at night because it makes noise\",\n",
    "                    \"This would impact my daily routine\"\n",
    "                ]\n",
    "            },\n",
    "            \"technical_knowledge\": {\n",
    "                \"description\": \"Discussion showing understanding of how appliances work, system operations, equipment specifications, or technical aspects of energy use.\",\n",
    "                \"examples\": [\n",
    "                    \"Heat pumps are more efficient than traditional HVAC systems like furnaces\",\n",
    "                    \"Smart thermostats can optimize energy use through scheduling and sensors\",\n",
    "                    \"Variable-speed pumps adjust their speed based on demand\",\n",
    "                    \"Microwaves provide quick heating but can be inefficient for large meals\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # === CONVERSATION INTERACTION PATTERNS (Updated for Experimental Context) ===\n",
    "        self.engagement_patterns = {\n",
    "            \"information_seeking\": {\n",
    "                \"description\": \"Directive speech acts aimed at obtaining information, clarification, or elaboration.\",\n",
    "                \"examples\": [\n",
    "                    \"Can you explain that in more detail?\",\n",
    "                    \"How does that work?\",\n",
    "                    \"What are the specific steps to implement this?\"\n",
    "                ],\n",
    "                \"theoretical_basis\": \"Searle's Directives - utterances intended to get the listener to provide information or explanation\"\n",
    "            },\n",
    "            \"constraint_articulation\": {\n",
    "                \"description\": \"Assertive speech acts that state personal limitations, boundaries, or situational constraints.\",\n",
    "                \"examples\": [\n",
    "                    \"I'm sensitive to being cold\",\n",
    "                    \"That's not feasible for my situation\",\n",
    "                    \"I don't want to change my mealtime\"\n",
    "                ],\n",
    "                \"theoretical_basis\": \"Searle's Assertives - statements that convey information about the speaker's situation or limitations\"\n",
    "            },\n",
    "            \"solution_evaluation\": {\n",
    "                \"description\": \"Assertive speech acts that express judgments, assessments, or evaluations of proposed solutions.\",\n",
    "                \"examples\": [\n",
    "                    \"The proposed method seems reasonable\",\n",
    "                    \"I think the oven idea makes sense\",\n",
    "                    \"Electric vehicle charging at night is an excellent idea\",\n",
    "                    \"This would be effective for my situation\"\n",
    "                ],\n",
    "                \"theoretical_basis\": \"Searle's Assertives - statements that assess or evaluate the truth, feasibility, or value of proposals\"\n",
    "            },\n",
    "            \"commitment_expression\": {\n",
    "                \"description\": \"Commissive speech acts that express willingness to try solutions or commit to behavioral changes.\",\n",
    "                \"examples\": [\n",
    "                    \"I'm willing to adjust my thermostat settings\",\n",
    "                    \"I can reschedule when I run my dishwasher\",\n",
    "                    \"I'll try using ceiling fans to comfort myself\",\n",
    "                    \"I'll implement this change gradually\"\n",
    "                ],\n",
    "                \"theoretical_basis\": \"Searle's Commissives - utterances that commit the speaker to future courses of action\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def analyze_concepts_with_llm(self, conversation_text: str, participant: str = \"both\", max_retries: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze conversation for energy concepts using LLM contextual understanding with experimental context awareness.\n",
    "        \n",
    "        Args:\n",
    "            conversation_text (str): The conversation text to analyze\n",
    "            participant (str): Which participant to analyze ('user', 'assistant', or 'both')\n",
    "            max_retries (int): Maximum number of retry attempts for API calls\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Any]: Analysis results with detected concepts, confidence, evidence, and reasoning\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create comprehensive prompt for concept detection\n",
    "        prompt = self._create_analysis_prompt(conversation_text, participant)\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                print(f\"🧠 Analyzing concepts with {self.analysis_model} (experimental context-aware)...\")\n",
    "                \n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.analysis_model,\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"system\", \n",
    "                            \"content\": \"You are an expert analyst specializing in home energy efficiency conversations in experimental settings. You understand that users are provided with energy data by researchers, and you distinguish between authentic user engagement and data-enhanced assistant analysis. Analyze conversations for specific energy-related concepts with high accuracy and provide detailed explanations.\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\", \n",
    "                            \"content\": prompt\n",
    "                        }\n",
    "                    ],\n",
    "                    temperature=0.1,  # Low temperature for consistent analysis\n",
    "                    response_format={\"type\": \"json_object\"}  # Ensure JSON response\n",
    "                )\n",
    "                \n",
    "                # Parse the response\n",
    "                analysis_result = json.loads(response.choices[0].message.content)\n",
    "                return self._process_llm_response(analysis_result, participant)\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"⚠️ JSON parsing error (attempt {attempt + 1}): {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(1)\n",
    "                else:\n",
    "                    print(\"❌ Failed to parse LLM response after multiple attempts\")\n",
    "                    return self._create_empty_response(participant)\n",
    "                    \n",
    "            except openai.RateLimitError:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = (2 ** attempt)\n",
    "                    print(f\"⏳ Rate limit hit, waiting {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    raise\n",
    "                    \n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"⚠️ API error (attempt {attempt + 1}): {e}\")\n",
    "                    time.sleep(1)\n",
    "                else:\n",
    "                    raise\n",
    "    \n",
    "    def _create_analysis_prompt(self, conversation_text: str, participant: str) -> str:\n",
    "        \"\"\"Create a structured prompt for LLM concept analysis with experimental context awareness.\"\"\"\n",
    "        \n",
    "        # Create concept descriptions for the prompt\n",
    "        energy_concepts_desc = \"\\n\".join([\n",
    "            f\"- {name.replace('_', ' ').title()}: {info['description']}\"\n",
    "            for name, info in self.energy_concepts.items()\n",
    "        ])\n",
    "        \n",
    "        engagement_patterns_desc = \"\\n\".join([\n",
    "            f\"- {name.replace('_', ' ').title()}: {info['description']}\"\n",
    "            for name, info in self.engagement_patterns.items()\n",
    "        ])\n",
    "        \n",
    "        participant_instruction = {\n",
    "            \"user\": \"Focus only on what the user (human participant) says and discusses. Remember that users have been provided with energy data by researchers, so focus on their authentic engagement, personal constraints, preferences, and how they apply or respond to information rather than their data recall ability.\",\n",
    "            \"assistant\": \"Focus only on what ChatGPT/assistant says and discusses. Evaluate how well the assistant analyzes provided experimental data, adds domain expertise, and provides practical guidance beyond just restating data.\",\n",
    "            \"both\": \"Analyze the entire conversation including both participants, but apply different criteria for user (authentic engagement) vs assistant (data analysis quality).\"\n",
    "        }[participant]\n",
    "        \n",
    "        # Create scoring framework description with participant-specific criteria\n",
    "        scoring_framework = \"\"\n",
    "        for factor_name, factor_info in self.scoring_factors.items():\n",
    "            scoring_framework += f\"\\n{factor_name.upper()} (Weight: {factor_info['weight']}) - {factor_info['description']}:\\n\"\n",
    "            \n",
    "            if participant in [\"user\", \"both\"]:\n",
    "                scoring_framework += \"USER CRITERIA:\\n\"\n",
    "                for score, description in factor_info['user_criteria'].items():\n",
    "                    scoring_framework += f\"  {score}: {description}\\n\"\n",
    "            \n",
    "            if participant in [\"assistant\", \"both\"]:\n",
    "                scoring_framework += \"ASSISTANT CRITERIA:\\n\"\n",
    "                for score, description in factor_info['assistant_criteria'].items():\n",
    "                    scoring_framework += f\"  {score}: {description}\\n\"\n",
    "            \n",
    "            scoring_framework += \"\\n\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "Please analyze this conversation for the following concepts using an experimental context-aware multi-factor scoring approach. \n",
    "\n",
    "IMPORTANT EXPERIMENTAL CONTEXT:\n",
    "- This is an experimental setting where users have been provided with energy data files by researchers\n",
    "- Users are not expected to know specific energy data from memory\n",
    "- Focus on AUTHENTIC USER ENGAGEMENT vs DATA-ENHANCED ASSISTANT ANALYSIS\n",
    "- Distinguish between personal insights/constraints and data-driven analysis\n",
    "\n",
    "{participant_instruction}\n",
    "\n",
    "ENERGY CONCEPTS TO DETECT:\n",
    "{energy_concepts_desc}\n",
    "\n",
    "INTERACTION PATTERNS TO DETECT:\n",
    "{engagement_patterns_desc}\n",
    "\n",
    "EXPERIMENTAL CONTEXT-AWARE SCORING FRAMEWORK:\n",
    "{scoring_framework}\n",
    "\n",
    "CONVERSATION TO ANALYZE:\n",
    "{conversation_text}\n",
    "\n",
    "For each concept (both energy concepts and interaction patterns):\n",
    "\n",
    "1. Determine if the concept was meaningfully discussed (not just mentioned in passing)\n",
    "2. Apply the appropriate criteria (USER vs ASSISTANT) based on who is being analyzed\n",
    "3. Score each of the 4 factors (explicitness, depth, consideration, evidence) from 0.0 to 1.0\n",
    "4. Calculate weighted confidence: (explicitness × 0.30) + (depth × 0.25) + (consideration × 0.25) + (evidence × 0.20)\n",
    "5. Provide specific evidence from the conversation\n",
    "6. Give brief reasoning for each factor score, noting whether it's authentic engagement or data analysis\n",
    "\n",
    "Provide your analysis in this exact JSON format:\n",
    "{{\n",
    "    \"energy_concepts\": {{\n",
    "        \"energy_consumption\": {{\n",
    "            \"detected\": true/false,\n",
    "            \"factor_scores\": {{\n",
    "                \"explicitness\": {{\n",
    "                    \"score\": 0.0-1.0,\n",
    "                    \"justification\": \"Brief explanation considering experimental context\"\n",
    "                }},\n",
    "                \"depth\": {{\n",
    "                    \"score\": 0.0-1.0,\n",
    "                    \"justification\": \"Brief explanation distinguishing authentic vs data-driven depth\"\n",
    "                }},\n",
    "                \"consideration\": {{\n",
    "                    \"score\": 0.0-1.0,\n",
    "                    \"justification\": \"Brief explanation of role in participant's conversation focus\"\n",
    "                }},\n",
    "                \"evidence\": {{\n",
    "                    \"score\": 0.0-1.0,\n",
    "                    \"justification\": \"Brief explanation of evidence type and quality\"\n",
    "                }}\n",
    "            }},\n",
    "            \"weighted_confidence\": \"calculated weighted score\",\n",
    "            \"evidence_quote\": \"Direct quote from conversation supporting detection\",\n",
    "            \"overall_reasoning\": \"Summary considering experimental context and authentic vs data-driven analysis\",\n",
    "            \"analysis_type\": \"authentic_engagement/data_analysis/mixed\"\n",
    "        }},\n",
    "        ...other energy concepts...\n",
    "    }},\n",
    "    \"engagement_patterns\": {{\n",
    "        ...interaction patterns with same structure...\n",
    "    }},\n",
    "    \"summary\": {{\n",
    "        \"total_energy_concepts_detected\": number,\n",
    "        \"total_engagement_patterns_detected\": number,\n",
    "        \"avg_confidence_energy\": 0.0-1.0,\n",
    "        \"avg_confidence_interaction\": 0.0-1.0,\n",
    "        \"conversation_focus\": \"Brief description of main topics\",\n",
    "        \"scoring_method\": \"experimental_context_aware_multi_factor\",\n",
    "        \"participant_role\": \"user/assistant/both\"\n",
    "    }}\n",
    "}}\n",
    "\n",
    "Be thorough and precise. Apply experimental context-aware criteria consistently. Only mark concepts as detected if they meet a reasonable threshold (typically weighted_confidence ≥ 0.4). Remember to distinguish between authentic user engagement and data-enhanced assistant analysis.\n",
    "\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def _process_llm_response(self, analysis_result: Dict[str, Any], participant: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process and validate LLM analysis response with experimental context awareness.\"\"\"\n",
    "        \n",
    "        processed_result = {\n",
    "            \"energy_concepts\": {},\n",
    "            \"engagement_patterns\": {},\n",
    "            \"summary\": analysis_result.get(\"summary\", {}),\n",
    "            \"analysis_method\": \"experimental_context_aware_llm_multi_factor\",\n",
    "            \"participant_analyzed\": participant\n",
    "        }\n",
    "        \n",
    "        # Process energy concepts\n",
    "        energy_results = analysis_result.get(\"energy_concepts\", {})\n",
    "        for concept_name in self.energy_concepts.keys():\n",
    "            concept_result = energy_results.get(concept_name, {})\n",
    "            \n",
    "            # Extract factor scores\n",
    "            factor_scores = concept_result.get(\"factor_scores\", {})\n",
    "            \n",
    "            # Calculate or extract weighted confidence\n",
    "            weighted_confidence = concept_result.get(\"weighted_confidence\", 0.0)\n",
    "            if isinstance(weighted_confidence, str):\n",
    "                # If it's a string, try to extract the number\n",
    "                try:\n",
    "                    weighted_confidence = float(re.findall(r'\\d+\\.?\\d*', weighted_confidence)[0])\n",
    "                except:\n",
    "                    weighted_confidence = 0.0\n",
    "            \n",
    "            # If weighted_confidence is 0 or missing, calculate it from factor scores\n",
    "            if weighted_confidence == 0.0 and factor_scores:\n",
    "                explicitness = factor_scores.get(\"explicitness\", {}).get(\"score\", 0.0)\n",
    "                depth = factor_scores.get(\"depth\", {}).get(\"score\", 0.0)\n",
    "                consideration = factor_scores.get(\"consideration\", {}).get(\"score\", 0.0)\n",
    "                evidence = factor_scores.get(\"evidence\", {}).get(\"score\", 0.0)\n",
    "                \n",
    "                weighted_confidence = (\n",
    "                    explicitness * self.scoring_factors[\"explicitness\"][\"weight\"] +\n",
    "                    depth * self.scoring_factors[\"depth\"][\"weight\"] +\n",
    "                    consideration * self.scoring_factors[\"consideration\"][\"weight\"] +\n",
    "                    evidence * self.scoring_factors[\"evidence\"][\"weight\"]\n",
    "                )\n",
    "            \n",
    "            processed_result[\"energy_concepts\"][f\"energy_{concept_name}\"] = {\n",
    "                \"detected\": concept_result.get(\"detected\", False),\n",
    "                \"confidence\": float(weighted_confidence),\n",
    "                \"factor_scores\": {\n",
    "                    \"explicitness\": {\n",
    "                        \"score\": float(factor_scores.get(\"explicitness\", {}).get(\"score\", 0.0)),\n",
    "                        \"justification\": factor_scores.get(\"explicitness\", {}).get(\"justification\", \"\")\n",
    "                    },\n",
    "                    \"depth\": {\n",
    "                        \"score\": float(factor_scores.get(\"depth\", {}).get(\"score\", 0.0)),\n",
    "                        \"justification\": factor_scores.get(\"depth\", {}).get(\"justification\", \"\")\n",
    "                    },\n",
    "                    \"consideration\": {\n",
    "                        \"score\": float(factor_scores.get(\"consideration\", {}).get(\"score\", 0.0)),\n",
    "                        \"justification\": factor_scores.get(\"consideration\", {}).get(\"justification\", \"\")\n",
    "                    },\n",
    "                    \"evidence\": {\n",
    "                        \"score\": float(factor_scores.get(\"evidence\", {}).get(\"score\", 0.0)),\n",
    "                        \"justification\": factor_scores.get(\"evidence\", {}).get(\"justification\", \"\")\n",
    "                    }\n",
    "                },\n",
    "                \"evidence_quote\": concept_result.get(\"evidence_quote\", \"\"),\n",
    "                \"overall_reasoning\": concept_result.get(\"overall_reasoning\", \"\"),\n",
    "                \"analysis_type\": concept_result.get(\"analysis_type\", \"unknown\"),\n",
    "                \"scoring_method\": \"experimental_context_aware_multi_factor\",\n",
    "                \"participant_analyzed\": participant,\n",
    "                \"original_llm_detection\": concept_result.get(\"original_llm_detection\", False),\n",
    "                \"threshold_applied\": concept_result.get(\"threshold_applied\", 0.4),\n",
    "                \"threshold_override\": concept_result.get(\"threshold_override\", False)\n",
    "            }\n",
    "        \n",
    "        # Process interaction patterns (similar structure)\n",
    "        interaction_results = analysis_result.get(\"engagement_patterns\", {})\n",
    "        for pattern_name in self.engagement_patterns.keys():\n",
    "            pattern_result = interaction_results.get(pattern_name, {})\n",
    "            \n",
    "            # Extract factor scores\n",
    "            factor_scores = pattern_result.get(\"factor_scores\", {})\n",
    "            \n",
    "            # Calculate or extract weighted confidence\n",
    "            weighted_confidence = pattern_result.get(\"weighted_confidence\", 0.0)\n",
    "            if isinstance(weighted_confidence, str):\n",
    "                try:\n",
    "                    weighted_confidence = float(re.findall(r'\\d+\\.?\\d*', weighted_confidence)[0])\n",
    "                except:\n",
    "                    weighted_confidence = 0.0\n",
    "            \n",
    "            # Calculate if missing\n",
    "            if weighted_confidence == 0.0 and factor_scores:\n",
    "                explicitness = factor_scores.get(\"explicitness\", {}).get(\"score\", 0.0)\n",
    "                depth = factor_scores.get(\"depth\", {}).get(\"score\", 0.0)\n",
    "                consideration = factor_scores.get(\"consideration\", {}).get(\"score\", 0.0)\n",
    "                evidence = factor_scores.get(\"evidence\", {}).get(\"score\", 0.0)\n",
    "                \n",
    "                weighted_confidence = (\n",
    "                    explicitness * self.scoring_factors[\"explicitness\"][\"weight\"] +\n",
    "                    depth * self.scoring_factors[\"depth\"][\"weight\"] +\n",
    "                    consideration * self.scoring_factors[\"consideration\"][\"weight\"] +\n",
    "                    evidence * self.scoring_factors[\"evidence\"][\"weight\"]\n",
    "                )\n",
    "            \n",
    "            processed_result[\"engagement_patterns\"][f\"interaction_{pattern_name}\"] = {\n",
    "                \"detected\": pattern_result.get(\"detected\", False),\n",
    "                \"confidence\": float(weighted_confidence),\n",
    "                \"factor_scores\": {\n",
    "                    \"explicitness\": {\n",
    "                        \"score\": float(factor_scores.get(\"explicitness\", {}).get(\"score\", 0.0)),\n",
    "                        \"justification\": factor_scores.get(\"explicitness\", {}).get(\"justification\", \"\")\n",
    "                    },\n",
    "                    \"depth\": {\n",
    "                        \"score\": float(factor_scores.get(\"depth\", {}).get(\"score\", 0.0)),\n",
    "                        \"justification\": factor_scores.get(\"depth\", {}).get(\"justification\", \"\")\n",
    "                    },\n",
    "                    \"consideration\": {\n",
    "                        \"score\": float(factor_scores.get(\"consideration\", {}).get(\"score\", 0.0)),\n",
    "                        \"justification\": factor_scores.get(\"consideration\", {}).get(\"justification\", \"\")\n",
    "                    },\n",
    "                    \"evidence\": {\n",
    "                        \"score\": float(factor_scores.get(\"evidence\", {}).get(\"score\", 0.0)),\n",
    "                        \"justification\": factor_scores.get(\"evidence\", {}).get(\"justification\", \"\")\n",
    "                    }\n",
    "                },\n",
    "                \"evidence_quote\": pattern_result.get(\"evidence_quote\", \"\"),\n",
    "                \"overall_reasoning\": pattern_result.get(\"overall_reasoning\", \"\"),\n",
    "                \"analysis_type\": pattern_result.get(\"analysis_type\", \"unknown\"),\n",
    "                \"scoring_method\": \"experimental_context_aware_multi_factor\",\n",
    "                \"participant_analyzed\": participant\n",
    "            }\n",
    "        \n",
    "        return processed_result\n",
    "    \n",
    "    def _create_empty_response(self, participant: str) -> Dict[str, Any]:\n",
    "        \"\"\"Create empty response structure when LLM analysis fails.\"\"\"\n",
    "        \n",
    "        empty_factor_scores = {\n",
    "            \"explicitness\": {\"score\": 0.0, \"justification\": \"Analysis failed\"},\n",
    "            \"depth\": {\"score\": 0.0, \"justification\": \"Analysis failed\"},\n",
    "            \"consideration\": {\"score\": 0.0, \"justification\": \"Analysis failed\"},\n",
    "            \"evidence\": {\"score\": 0.0, \"justification\": \"Analysis failed\"}\n",
    "        }\n",
    "        \n",
    "        empty_result = {\n",
    "            \"energy_concepts\": {},\n",
    "            \"engagement_patterns\": {},\n",
    "            \"summary\": {\n",
    "                \"total_energy_concepts_detected\": 0,\n",
    "                \"total_engagement_patterns_detected\": 0,\n",
    "                \"avg_confidence_energy\": 0.0,\n",
    "                \"avg_confidence_interaction\": 0.0,\n",
    "                \"conversation_focus\": \"Analysis failed\",\n",
    "                \"scoring_method\": \"experimental_context_aware_multi_factor\",\n",
    "                \"participant_role\": participant\n",
    "            },\n",
    "            \"analysis_method\": \"experimental_context_aware_llm_failed\",\n",
    "            \"participant_analyzed\": participant\n",
    "        }\n",
    "        \n",
    "        # Add empty entries for all concepts\n",
    "        for concept_name in self.energy_concepts.keys():\n",
    "            empty_result[\"energy_concepts\"][f\"energy_{concept_name}\"] = {\n",
    "                \"detected\": False,\n",
    "                \"confidence\": 0.0,\n",
    "                \"factor_scores\": empty_factor_scores.copy(),\n",
    "                \"evidence_quote\": \"\",\n",
    "                \"overall_reasoning\": \"Analysis failed\",\n",
    "                \"analysis_type\": \"unknown\",\n",
    "                \"scoring_method\": \"experimental_context_aware_multi_factor\",\n",
    "                \"participant_analyzed\": participant\n",
    "            }\n",
    "        \n",
    "        for pattern_name in self.engagement_patterns.keys():\n",
    "            empty_result[\"engagement_patterns\"][f\"interaction_{pattern_name}\"] = {\n",
    "                \"detected\": False,\n",
    "                \"confidence\": 0.0,\n",
    "                \"factor_scores\": empty_factor_scores.copy(),\n",
    "                \"evidence_quote\": \"\",\n",
    "                \"overall_reasoning\": \"Analysis failed\",\n",
    "                \"analysis_type\": \"unknown\",\n",
    "                \"scoring_method\": \"experimental_context_aware_multi_factor\",\n",
    "                \"participant_analyzed\": participant\n",
    "            }\n",
    "        \n",
    "        return empty_result\n",
    "    \n",
    "    def comprehensive_analysis(self, file_path: str, detection_threshold: float = 0.4) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Perform complete experimental context-aware LLM-based analysis of a single conversation file.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to conversation JSON file\n",
    "            detection_threshold (float): Confidence threshold for concept detection (0.0-1.0)\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Any]: Complete analysis results with experimental context awareness\n",
    "        \"\"\"\n",
    "        print(f\"📖 Loading conversation from: {file_path}\")\n",
    "        \n",
    "        # Load conversation data\n",
    "        df = self.load_conversation(file_path)\n",
    "        \n",
    "        # Extract subject number from folder path\n",
    "        import os\n",
    "        folder_name = os.path.basename(os.path.dirname(file_path))\n",
    "        if folder_name.isdigit():\n",
    "            subject_id = folder_name # Use folder number (e.g., \"002\")\n",
    "        else:\n",
    "            # Fallback to original method if folder structure is different\n",
    "            subject_id = df['subject_id'].iloc[0]\n",
    "\n",
    "        # Prepare conversation text for analysis\n",
    "        conversation_text = self._format_conversation_for_analysis(df)\n",
    "        \n",
    "        # Analyze user messages separately with experimental context awareness\n",
    "        user_texts = df[df['role'] == 'user']['text'].tolist()\n",
    "        user_conversation = \"\\n\".join([f\"User: {text}\" for text in user_texts])\n",
    "        \n",
    "        # Analyze GPT messages separately with experimental context awareness\n",
    "        gpt_texts = df[df['role'] == 'assistant']['text'].tolist()\n",
    "        gpt_conversation = \"\\n\".join([f\"Assistant: {text}\" for text in gpt_texts])\n",
    "        \n",
    "        print(\"🧠 Analyzing user concepts (authentic engagement focus)...\")\n",
    "        user_analysis = self.analyze_concepts_with_llm(user_conversation, \"user\")\n",
    "        \n",
    "        print(\"🧠 Analyzing GPT concepts (data analysis quality focus)...\")\n",
    "        gpt_analysis = self.analyze_concepts_with_llm(gpt_conversation, \"assistant\")\n",
    "\n",
    "        print(\"🔍 Applying detection threshold to user analysis...\")\n",
    "        user_analysis = self._apply_detection_threshold(user_analysis, detection_threshold)\n",
    "\n",
    "        print(\"🔍 Applying detection threshold to GPT analysis...\")\n",
    "        gpt_analysis = self._apply_detection_threshold(gpt_analysis, detection_threshold)\n",
    "             \n",
    "        print(\"📊 Analyzing engagement patterns...\")\n",
    "        user_engagement = self.analyze_user_engagement(df)\n",
    "        conversation_flow = self.analyze_conversation_flow(df)\n",
    "        \n",
    "        # Compile results with experimental context awareness\n",
    "        results = {\n",
    "            'subject_id': subject_id,\n",
    "            'analysis_framework': 'experimental_context_aware_multi_factor',\n",
    "            'conversation_metadata': {\n",
    "                'total_turns': len(df),\n",
    "                'user_turns': len(df[df['role'] == 'user']),\n",
    "                'gpt_turns': len(df[df['role'] == 'assistant']),\n",
    "                'total_words': df['word_count'].sum(),\n",
    "                'experimental_context': 'users_provided_energy_data'\n",
    "            },\n",
    "            'user_concepts': {**user_analysis['energy_concepts'], **user_analysis['engagement_patterns']},\n",
    "            'gpt_concepts': {**gpt_analysis['energy_concepts'], **gpt_analysis['engagement_patterns']},\n",
    "            'user_engagement': user_engagement,\n",
    "            'conversation_flow': conversation_flow,\n",
    "            'concept_attribution': self._analyze_concept_attribution(user_analysis, gpt_analysis),\n",
    "            'analysis_summary': {\n",
    "                'user_analysis_summary': user_analysis.get('summary', {}),\n",
    "                'gpt_analysis_summary': gpt_analysis.get('summary', {}),\n",
    "                'analysis_method': 'experimental_context_aware_llm',\n",
    "                'scoring_framework': 'authentic_engagement_vs_data_analysis'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def load_conversation(self, file_path):\n",
    "        \"\"\"\n",
    "        Load and parse conversation data from JSON file into structured DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to JSON file containing conversation data\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Structured conversation data\n",
    "        \"\"\"\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            conversation_data = json.load(f)\n",
    "        \n",
    "        records = []\n",
    "        subject_id = file_path.split('/')[-1].replace('.json', '')\n",
    "        \n",
    "        for turn_idx, turn in enumerate(conversation_data):\n",
    "            for key, value in turn.items():\n",
    "                if key.lower().startswith(\"you said\"):\n",
    "                    records.append({\n",
    "                        'subject_id': subject_id, \n",
    "                        'turn': turn_idx + 1,\n",
    "                        'role': 'user', \n",
    "                        'text': value,\n",
    "                        'word_count': len(value.split()),\n",
    "                        'char_count': len(value)\n",
    "                    })\n",
    "                elif key.lower().startswith(\"chatgpt said\"):\n",
    "                    records.append({\n",
    "                        'subject_id': subject_id, \n",
    "                        'turn': turn_idx + 1,\n",
    "                        'role': 'assistant', \n",
    "                        'text': value,\n",
    "                        'word_count': len(value.split()),\n",
    "                        'char_count': len(value)\n",
    "                    })\n",
    "        \n",
    "        return pd.DataFrame(records)\n",
    "    \n",
    "    def analyze_user_engagement(self, df):\n",
    "        \"\"\"Analyze user engagement patterns throughout the conversation.\"\"\"\n",
    "        \n",
    "        user_data = df[df['role'] == 'user']\n",
    "        gpt_data = df[df['role'] == 'assistant']\n",
    "        \n",
    "        total_prompts = len(user_data)\n",
    "        avg_prompt_length = user_data['word_count'].mean()\n",
    "        prompt_length_std = user_data['word_count'].std()\n",
    "        total_user_words = user_data['word_count'].sum()\n",
    "        total_gpt_words = gpt_data['word_count'].sum()\n",
    "        \n",
    "        if total_gpt_words > 0:\n",
    "            prompt_response_ratio = total_user_words / total_gpt_words\n",
    "        else:\n",
    "            prompt_response_ratio = 0.0\n",
    "        \n",
    "        return {\n",
    "            'total_prompts': total_prompts,\n",
    "            'avg_prompt_length': avg_prompt_length,\n",
    "            'prompt_length_std': prompt_length_std,\n",
    "            'total_user_words': total_user_words,\n",
    "            'total_gpt_words': total_gpt_words,\n",
    "            'prompt_response_ratio': prompt_response_ratio,\n",
    "        }\n",
    "    \n",
    "    def analyze_conversation_flow(self, df):\n",
    "        \"\"\"Analyze basic conversation flow without requiring embeddings.\"\"\"\n",
    "        \n",
    "        user_texts = df[df['role'] == 'user']['text'].tolist()\n",
    "        \n",
    "        return {\n",
    "            'conversation_length': len(df),\n",
    "            'user_prompt_count': len(user_texts),\n",
    "            'avg_user_prompt_length': df[df['role'] == 'user']['word_count'].mean(),\n",
    "            'conversation_turns': len(df[df['role'] == 'user']) + len(df[df['role'] == 'assistant'])\n",
    "        }\n",
    "    \n",
    "    def _apply_detection_threshold(self, analysis_result: Dict[str, Any], threshold: float) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Apply consistent detection threshold to override LLM detection decisions.\n",
    "        \n",
    "        Args:\n",
    "            analysis_result (Dict): Analysis result from LLM\n",
    "            threshold (float): Confidence threshold for detection\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Analysis result with threshold-based detection decisions\n",
    "        \"\"\"\n",
    "        categories_to_process = ['energy_concepts', 'engagement_patterns']\n",
    "        \n",
    "        for category in categories_to_process:\n",
    "            if category in analysis_result:\n",
    "                for concept_name, concept_data in analysis_result[category].items():\n",
    "                    confidence = concept_data.get('confidence', 0.0)\n",
    "                    original_detection = concept_data.get('detected', False)\n",
    "                    \n",
    "                    # Apply threshold-based detection\n",
    "                    threshold_detection = confidence >= threshold\n",
    "                    \n",
    "                    # Update detection status\n",
    "                    concept_data['detected'] = threshold_detection\n",
    "                    concept_data['original_llm_detection'] = original_detection\n",
    "                    concept_data['threshold_applied'] = threshold\n",
    "                    concept_data['threshold_override'] = (original_detection != threshold_detection)\n",
    "                    \n",
    "                    # Add threshold info to reasoning\n",
    "                    if concept_data.get('overall_reasoning'):\n",
    "                        concept_data['overall_reasoning'] += f\" [Threshold {threshold}: {'DETECTED' if threshold_detection else 'NOT DETECTED'}]\"\n",
    "        \n",
    "        return analysis_result\n",
    "\n",
    "    def _format_conversation_for_analysis(self, df: pd.DataFrame) -> str:\n",
    "        \"\"\"Format conversation DataFrame into text for LLM analysis.\"\"\"\n",
    "        \n",
    "        conversation_parts = []\n",
    "        for _, row in df.iterrows():\n",
    "            if row['role'] == 'user':\n",
    "                conversation_parts.append(f\"User: {row['text']}\")\n",
    "            elif row['role'] == 'assistant':\n",
    "                conversation_parts.append(f\"Assistant: {row['text']}\")\n",
    "        \n",
    "        return \"\\n\\n\".join(conversation_parts)\n",
    "       \n",
    "    def _analyze_concept_attribution(self, user_analysis: Dict, gpt_analysis: Dict) -> Dict[str, str]:\n",
    "        \"\"\"Analyze which participant introduced each concept.\"\"\"\n",
    "        \n",
    "        attribution = {}\n",
    "        \n",
    "        all_concepts = set()\n",
    "        if 'energy_concepts' in user_analysis:\n",
    "            all_concepts.update(user_analysis['energy_concepts'].keys())\n",
    "        if 'engagement_patterns' in user_analysis:\n",
    "            all_concepts.update(user_analysis['engagement_patterns'].keys())\n",
    "        if 'energy_concepts' in gpt_analysis:\n",
    "            all_concepts.update(gpt_analysis['energy_concepts'].keys())\n",
    "        if 'engagement_patterns' in gpt_analysis:\n",
    "            all_concepts.update(gpt_analysis['engagement_patterns'].keys())\n",
    "        \n",
    "        for concept in all_concepts:\n",
    "            user_detected = self._get_concept_detection(user_analysis, concept)\n",
    "            gpt_detected = self._get_concept_detection(gpt_analysis, concept)\n",
    "            \n",
    "            if user_detected and gpt_detected:\n",
    "                attribution[concept] = \"both\"\n",
    "            elif user_detected:\n",
    "                attribution[concept] = \"user_guided\"\n",
    "            elif gpt_detected:\n",
    "                attribution[concept] = \"gpt_introduced\"\n",
    "            else:\n",
    "                attribution[concept] = \"not_detected\"\n",
    "        \n",
    "        return attribution\n",
    "    \n",
    "    def _get_concept_detection(self, analysis: Dict, concept: str) -> bool:\n",
    "        \"\"\"Helper to get concept detection status from analysis result.\"\"\"\n",
    "        \n",
    "        for category in ['energy_concepts', 'engagement_patterns']:\n",
    "            if category in analysis and concept in analysis[category]:\n",
    "                return analysis[category][concept].get('detected', False)\n",
    "        return False\n",
    "    \n",
    "    def export_results(self, results: Dict[str, Any], output_folder: str = \"Result\"):\n",
    "        \"\"\"Export analysis results to JSON file.\"\"\"\n",
    "        \n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        subject_id = results['subject_id']\n",
    "        number_match = re.search(r'(\\d+)', subject_id)\n",
    "        if number_match:\n",
    "            subject_number = number_match.group(1).zfill(3)\n",
    "        else:\n",
    "            subject_number = \"001\"\n",
    "        \n",
    "        output_file = os.path.join(output_folder, \"SCALE\", f\"llm_analysis_results_{subject_number}.json\")\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"💾 Results exported to: {output_file}\")\n",
    "    \n",
    "    def create_detailed_report(self, results: Dict[str, Any]) -> str:\n",
    "        \"\"\"Generate a detailed human-readable report with multi-factor explanations.\"\"\"\n",
    "        \n",
    "        report = f\"\"\"\n",
    "CONVERSATION ANALYSIS REPORT (Multi-Factor LLM Analysis)\n",
    "=======================================================\n",
    "\n",
    "Subject ID: {results['subject_id']}\n",
    "Analysis Method: Multi-Factor LLM Analysis using {self.analysis_model}\n",
    "Scoring Framework: Weighted combination of Explicitness (30%), Depth (25%), Consideration (25%), Evidence (20%)\n",
    "Total Conversation Turns: {results['conversation_metadata']['total_turns']}\n",
    "Total Words: {results['conversation_metadata']['total_words']}\n",
    "\n",
    "USER ENGAGEMENT ANALYSIS\n",
    "------------------------\n",
    "- Number of prompts: {results['user_engagement']['total_prompts']}\n",
    "- Average prompt length: {results['user_engagement']['avg_prompt_length']:.1f} words\n",
    "- Total user words: {results['user_engagement']['total_user_words']}\n",
    "- Total GPT words: {results['user_engagement']['total_gpt_words']}\n",
    "- Prompt-response ratio: {results['user_engagement']['prompt_response_ratio']:.3f}\n",
    "\n",
    "DETECTED ENERGY CONCEPTS (Multi-Factor Analysis)\n",
    "===============================================\n",
    "User-detected concepts:\n",
    "\"\"\"\n",
    "        \n",
    "        for concept, data in results['user_concepts'].items():\n",
    "            if concept.startswith('energy_') and data.get('detected', False):\n",
    "                concept_name = concept.replace('energy_', '').replace('_', ' ').title()\n",
    "                confidence = data.get('confidence', 0)\n",
    "                \n",
    "                report += f\"\\n✅ {concept_name} (Overall Confidence: {confidence:.3f})\\n\"\n",
    "                report += f\"   Evidence: \\\"{data.get('evidence_quote', 'N/A')[:120]}...\\\"\\n\"\n",
    "                report += f\"   Reasoning: {data.get('overall_reasoning', 'N/A')[:150]}...\\n\"\n",
    "                \n",
    "                # Add factor breakdown\n",
    "                factor_scores = data.get('factor_scores', {})\n",
    "                if factor_scores:\n",
    "                    report += f\"   Factor Breakdown:\\n\"\n",
    "                    for factor_name, factor_data in factor_scores.items():\n",
    "                        score = factor_data.get('score', 0)\n",
    "                        weight = self.scoring_factors.get(factor_name, {}).get('weight', 0)\n",
    "                        weighted_contribution = score * weight\n",
    "                        report += f\"     • {factor_name.title()}: {score:.2f} (×{weight:.2f} = {weighted_contribution:.3f}) - {factor_data.get('justification', '')[:80]}...\\n\"\n",
    "        \n",
    "        report += \"\\nGPT-detected concepts:\\n\"\n",
    "        for concept, data in results['gpt_concepts'].items():\n",
    "            if concept.startswith('energy_') and data.get('detected', False):\n",
    "                concept_name = concept.replace('energy_', '').replace('_', ' ').title()\n",
    "                confidence = data.get('confidence', 0)\n",
    "                \n",
    "                report += f\"\\n✅ {concept_name} (Overall Confidence: {confidence:.3f})\\n\"\n",
    "                report += f\"   Evidence: \\\"{data.get('evidence_quote', 'N/A')[:120]}...\\\"\\n\"\n",
    "                report += f\"   Reasoning: {data.get('overall_reasoning', 'N/A')[:150]}...\\n\"\n",
    "                \n",
    "                # Add factor breakdown\n",
    "                factor_scores = data.get('factor_scores', {})\n",
    "                if factor_scores:\n",
    "                    report += f\"   Factor Breakdown:\\n\"\n",
    "                    for factor_name, factor_data in factor_scores.items():\n",
    "                        score = factor_data.get('score', 0)\n",
    "                        weight = self.scoring_factors.get(factor_name, {}).get('weight', 0)\n",
    "                        weighted_contribution = score * weight\n",
    "                        report += f\"     • {factor_name.title()}: {score:.2f} (×{weight:.2f} = {weighted_contribution:.3f}) - {factor_data.get('justification', '')[:80]}...\\n\"\n",
    "        \n",
    "        report += f\"\\nDETECTED INTERACTION PATTERNS\\n\"\n",
    "        report += f\"=============================\\n\"\n",
    "        \n",
    "        for concept, data in results['user_concepts'].items():\n",
    "            if concept.startswith('interaction_') and data.get('detected', False):\n",
    "                concept_name = concept.replace('interaction_', '').replace('_', ' ').title()\n",
    "                confidence = data.get('confidence', 0)\n",
    "                \n",
    "                report += f\"\\n✅ {concept_name} (Confidence: {confidence:.3f})\\n\"\n",
    "                report += f\"   Evidence: \\\"{data.get('evidence_quote', 'N/A')[:100]}...\\\"\\n\"\n",
    "                \n",
    "                # Add factor breakdown for interaction patterns too\n",
    "                factor_scores = data.get('factor_scores', {})\n",
    "                if factor_scores:\n",
    "                    report += f\"   Factor Breakdown:\\n\"\n",
    "                    for factor_name, factor_data in factor_scores.items():\n",
    "                        score = factor_data.get('score', 0)\n",
    "                        report += f\"     • {factor_name.title()}: {score:.2f} - {factor_data.get('justification', '')[:60]}...\\n\"\n",
    "        \n",
    "        # Add analysis summary\n",
    "        if 'analysis_summary' in results:\n",
    "            user_summary = results['analysis_summary'].get('user_analysis_summary', {})\n",
    "            gpt_summary = results['analysis_summary'].get('gpt_analysis_summary', {})\n",
    "            \n",
    "            report += f\"\\nMULTI-FACTOR ANALYSIS SUMMARY\\n\"\n",
    "            report += f\"============================\\n\"\n",
    "            report += f\"User energy concepts detected: {user_summary.get('total_energy_concepts_detected', 0)}\\n\"\n",
    "            report += f\"GPT energy concepts detected: {gpt_summary.get('total_energy_concepts_detected', 0)}\\n\"\n",
    "            report += f\"Average user confidence: {user_summary.get('avg_confidence_energy', 0):.3f}\\n\"\n",
    "            report += f\"Average GPT confidence: {gpt_summary.get('avg_confidence_energy', 0):.3f}\\n\"\n",
    "            report += f\"Conversation focus: {user_summary.get('conversation_focus', 'Not specified')}\\n\"\n",
    "            report += f\"Scoring method: {user_summary.get('scoring_method', 'multi_factor_weighted')}\\n\"\n",
    "        \n",
    "        report += f\"\\nSCORING FRAMEWORK WEIGHTS\\n\"\n",
    "        report += f\"========================\\n\"\n",
    "        for factor_name, factor_info in self.scoring_factors.items():\n",
    "            report += f\"• {factor_name.title()}: {factor_info['weight']:.0%} - {factor_info['description']}\\n\"\n",
    "        \n",
    "        return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e38ac048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_single_conversation_llm(file_path: str, api_key: str = None, analysis_model: str = \"gpt-4o-mini\", detection_threshold: float = 0.4) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze a single conversation file using experimental context-aware LLM-based understanding.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the conversation JSON file\n",
    "        api_key (str): OpenAI API key (optional - will auto-load from .env file if not provided)\n",
    "        analysis_model (str): OpenAI chat model to use for analysis\n",
    "        detection_threshold (float): Confidence threshold for concept detection (0.0-1.0)\n",
    "                                   Higher values = more conservative detection\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, Any]: Complete analysis results with experimental context explanations\n",
    "    \"\"\"\n",
    "    try:\n",
    "        analyzer = LLMChatAnalyzer(api_key=api_key, analysis_model=analysis_model)\n",
    "        \n",
    "        results = analyzer.comprehensive_analysis(file_path, detection_threshold=detection_threshold)\n",
    "        \n",
    "        # Generate and display detailed report\n",
    "        detailed_report = analyzer.create_detailed_report(results)\n",
    "        print(detailed_report)\n",
    "        \n",
    "        # Export results\n",
    "        analyzer.export_results(results)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"✅ Experimental Context-Aware Multi-Factor LLM Analysis completed successfully!\")\n",
    "        print(f\"🧠 Model used: {analysis_model}\")\n",
    "        print(\"💾 Results exported to Result folder with subject numbering\")\n",
    "        print(\"📊 Includes experimental context-aware multi-factor confidence scoring\")\n",
    "        print(\"🔍 Distinguishes authentic user engagement from data-enhanced assistant analysis\")\n",
    "        print(\"🎯 Each concept scored with participant-appropriate criteria\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during experimental context-aware LLM analysis: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "631db7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Experimental Context-Aware Multi-Factor LLM-Based GPT Chat Analyzer Ready!\n",
      "📊 This version distinguishes authentic user engagement from data-enhanced assistant analysis\n",
      "🔑 Make sure you have a .env file with your OpenAI API key\n",
      "💰 Note: Experimental context-aware LLM analysis provides systematic confidence scores\n",
      "\\n📋 Experimental Context-Aware Scoring Framework:\n",
      "   • USER: Authentic engagement (personal insights, constraints, genuine questions)\n",
      "   • ASSISTANT: Data analysis quality (synthesis, domain expertise, practical guidance)\n",
      "   • Explicitness (30%): Appropriate terminology for participant role\n",
      "   • Depth (25%): Authentic engagement vs data analysis quality\n",
      "   • Consideration (25%): Role-appropriate conversation focus\n",
      "   • Evidence (20%): Personal examples vs quantitative analysis\n",
      "🧠 Initialized with OpenAI chat model: gpt-4o-mini\n",
      "🔑 API key loaded successfully from parameter\n",
      "📊 Using experimental context-aware multi-factor confidence scoring framework\n",
      "📖 Loading conversation from: ./Data/020/EntireConversation_extracted.json\n",
      "🧠 Analyzing user concepts (authentic engagement focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🧠 Analyzing GPT concepts (data analysis quality focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🔍 Applying detection threshold to user analysis...\n",
      "🔍 Applying detection threshold to GPT analysis...\n",
      "📊 Analyzing engagement patterns...\n",
      "\n",
      "CONVERSATION ANALYSIS REPORT (Multi-Factor LLM Analysis)\n",
      "=======================================================\n",
      "\n",
      "Subject ID: 020\n",
      "Analysis Method: Multi-Factor LLM Analysis using gpt-4o-mini\n",
      "Scoring Framework: Weighted combination of Explicitness (30%), Depth (25%), Consideration (25%), Evidence (20%)\n",
      "Total Conversation Turns: 10\n",
      "Total Words: 1401\n",
      "\n",
      "USER ENGAGEMENT ANALYSIS\n",
      "------------------------\n",
      "- Number of prompts: 5\n",
      "- Average prompt length: 16.2 words\n",
      "- Total user words: 81\n",
      "- Total GPT words: 1320\n",
      "- Prompt-response ratio: 0.061\n",
      "\n",
      "DETECTED ENERGY CONCEPTS (Multi-Factor Analysis)\n",
      "===============================================\n",
      "User-detected concepts:\n",
      "\n",
      "✅ Cost Awareness (Overall Confidence: 0.660)\n",
      "   Evidence: \"I want you to help me identify five behavioral changes that I make make to reduce my energy bills....\"\n",
      "   Reasoning: The user is actively engaged in discussing cost awareness, particularly in relation to behavioral changes to reduce bills. [Threshold 0.4: DETECTED]...\n",
      "   Factor Breakdown:\n",
      "     • Explicitness: 0.80 (×0.30 = 0.240) - The user mentions 'energy bills' and expresses a desire to reduce them, indicati...\n",
      "     • Depth: 0.60 (×0.25 = 0.150) - The user shows some understanding of the financial implications of their energy ...\n",
      "     • Consideration: 0.80 (×0.25 = 0.200) - The user's focus on reducing energy bills indicates that cost awareness is a sig...\n",
      "     • Evidence: 0.40 (×0.20 = 0.080) - The user references their energy bills but does not provide specific examples or...\n",
      "\n",
      "✅ Behavioral Change (Overall Confidence: 0.820)\n",
      "   Evidence: \"the behavioral changes should not compromise my comfort and also should not bring about the adoption of new technology....\"\n",
      "   Reasoning: The user is deeply engaged in discussing behavioral changes, emphasizing comfort and practicality in their approach. [Threshold 0.4: DETECTED]...\n",
      "   Factor Breakdown:\n",
      "     • Explicitness: 1.00 (×0.30 = 0.300) - The user explicitly asks for behavioral changes to reduce energy bills....\n",
      "     • Depth: 0.80 (×0.25 = 0.200) - The user articulates a clear desire for changes that do not compromise comfort, ...\n",
      "     • Consideration: 1.00 (×0.25 = 0.250) - Behavioral change is central to the user's inquiry, showing it significantly inf...\n",
      "     • Evidence: 0.60 (×0.20 = 0.120) - The user provides context about their comfort preferences, but lacks specific ex...\n",
      "\n",
      "✅ Comfort Association (Overall Confidence: 0.820)\n",
      "   Evidence: \"the behavioral changes should not compromise my comfort....\"\n",
      "   Reasoning: The user is actively engaged in discussing comfort, emphasizing its importance in their energy-related decisions. [Threshold 0.4: DETECTED]...\n",
      "   Factor Breakdown:\n",
      "     • Explicitness: 1.00 (×0.30 = 0.300) - The user explicitly mentions comfort in relation to behavioral changes....\n",
      "     • Depth: 0.80 (×0.25 = 0.200) - The user articulates a clear concern for comfort while seeking behavioral change...\n",
      "     • Consideration: 1.00 (×0.25 = 0.250) - Comfort is a primary consideration in the user's decision-making process....\n",
      "     • Evidence: 0.60 (×0.20 = 0.120) - The user provides context about their comfort preferences but lacks specific exa...\n",
      "\n",
      "GPT-detected concepts:\n",
      "\n",
      "✅ Consumption (Overall Confidence: 0.760)\n",
      "   Evidence: \"HVAC Usage Analysis Daily Usage: Average of 328 kWh/day with peaks reaching 403 kWh/day....\"\n",
      "   Reasoning: The assistant effectively analyzes energy consumption data, providing insights and recommendations based on the user's specific data, indicating a str...\n",
      "   Factor Breakdown:\n",
      "     • Explicitness: 0.80 (×0.30 = 0.240) - The assistant uses appropriate technical terms like 'HVAC usage' and 'pool pump ...\n",
      "     • Depth: 0.80 (×0.25 = 0.200) - The assistant provides a thorough analysis of HVAC and pool pump usage patterns,...\n",
      "     • Consideration: 0.80 (×0.25 = 0.200) - The concept of energy consumption is clearly integrated into the assistant's rec...\n",
      "     • Evidence: 0.60 (×0.20 = 0.120) - The assistant references specific energy data (e.g., kWh/day) but does not provi...\n",
      "\n",
      "✅ Cost Awareness (Overall Confidence: 0.760)\n",
      "   Evidence: \"The highest TOU rate period occurs in July between 2 PM and 8 PM at a rate of 26.1 cents per kWh....\"\n",
      "   Reasoning: The assistant effectively integrates cost awareness into its analysis, providing actionable insights based on the user's energy data. [Threshold 0.4: ...\n",
      "   Factor Breakdown:\n",
      "     • Explicitness: 0.80 (×0.30 = 0.240) - The assistant discusses TOU rates and potential savings using clear terminology....\n",
      "     • Depth: 0.80 (×0.25 = 0.200) - The assistant explores the financial implications of energy usage patterns and p...\n",
      "     • Consideration: 0.80 (×0.25 = 0.200) - Cost awareness is a central theme in the assistant's recommendations, focusing o...\n",
      "     • Evidence: 0.60 (×0.20 = 0.120) - The assistant provides specific TOU rates and estimated savings but lacks multip...\n",
      "\n",
      "✅ Behavioral Change (Overall Confidence: 0.760)\n",
      "   Evidence: \"I suggest the following strategies: Shift HVAC Cooling Earlier in the Day, Optimize Pool Pump Operation, etc....\"\n",
      "   Reasoning: The assistant provides a well-rounded analysis of behavioral changes, emphasizing practical steps based on the user's energy data. [Threshold 0.4: DET...\n",
      "   Factor Breakdown:\n",
      "     • Explicitness: 0.80 (×0.30 = 0.240) - The assistant explicitly outlines behavioral changes to reduce energy consumptio...\n",
      "     • Depth: 0.80 (×0.25 = 0.200) - The assistant provides a comprehensive list of strategies, detailing how each ca...\n",
      "     • Consideration: 0.80 (×0.25 = 0.200) - Behavioral change is a key focus of the assistant's recommendations, tailored to...\n",
      "     • Evidence: 0.60 (×0.20 = 0.120) - The assistant supports its recommendations with data-driven insights but does no...\n",
      "\n",
      "✅ Use Flexibility (Overall Confidence: 0.560)\n",
      "   Evidence: \"Optimize Pool Pump Operation: Your pool pump usage can be shifted to operate mostly between 12 AM and 8 AM....\"\n",
      "   Reasoning: The assistant acknowledges the flexibility of appliance usage but does not delve deeply into its implications, indicating a moderate level of analysis...\n",
      "   Factor Breakdown:\n",
      "     • Explicitness: 0.60 (×0.30 = 0.180) - The assistant discusses timing adjustments for appliance usage but lacks precise...\n",
      "     • Depth: 0.60 (×0.25 = 0.150) - The assistant touches on flexibility in appliance operation but does not explore...\n",
      "     • Consideration: 0.60 (×0.25 = 0.150) - Flexibility is mentioned in the context of scheduling but not deeply integrated ...\n",
      "     • Evidence: 0.40 (×0.20 = 0.080) - The assistant provides some data on usage patterns but lacks comprehensive suppo...\n",
      "\n",
      "✅ Use Frequency (Overall Confidence: 0.440)\n",
      "   Evidence: \"Pool Pump Usage Analysis Daily Usage: Average of 181 kWh/day, ranging up to 269 kWh/day....\"\n",
      "   Reasoning: The assistant provides some insights into usage frequency but does not explore it in depth, indicating a basic level of analysis. [Threshold 0.4: DETE...\n",
      "   Factor Breakdown:\n",
      "     • Explicitness: 0.60 (×0.30 = 0.180) - The assistant mentions frequency of appliance usage but does not use specific te...\n",
      "     • Depth: 0.40 (×0.25 = 0.100) - The analysis of frequency is basic and lacks detailed exploration....\n",
      "     • Consideration: 0.40 (×0.25 = 0.100) - Frequency is mentioned but not well integrated into the overall recommendations....\n",
      "     • Evidence: 0.40 (×0.20 = 0.080) - The assistant provides some data but lacks comprehensive evidence to support its...\n",
      "\n",
      "✅ Comfort Association (Overall Confidence: 0.440)\n",
      "   Evidence: \"Shifting some cooling efforts earlier in the day... can reduce your energy costs without sacrificing comfort....\"\n",
      "   Reasoning: The assistant acknowledges the importance of comfort in energy-saving strategies but does not explore it thoroughly, indicating a basic level of analy...\n",
      "   Factor Breakdown:\n",
      "     • Explicitness: 0.60 (×0.30 = 0.180) - The assistant discusses comfort in relation to HVAC usage but lacks precise term...\n",
      "     • Depth: 0.40 (×0.25 = 0.100) - The analysis of comfort is basic and does not explore various dimensions....\n",
      "     • Consideration: 0.40 (×0.25 = 0.100) - Comfort is mentioned but not deeply integrated into the recommendations....\n",
      "     • Evidence: 0.40 (×0.20 = 0.080) - The assistant provides some context but lacks comprehensive supporting evidence....\n",
      "\n",
      "✅ Technical Knowledge (Overall Confidence: 0.660)\n",
      "   Evidence: \"Your HVAC unit's energy consumption is highest between 5 PM and 9 PM....\"\n",
      "   Reasoning: The assistant shows a solid understanding of technical aspects related to energy use, providing valuable insights based on the user's data. [Threshold...\n",
      "   Factor Breakdown:\n",
      "     • Explicitness: 0.80 (×0.30 = 0.240) - The assistant demonstrates technical knowledge by discussing HVAC and pool pump ...\n",
      "     • Depth: 0.60 (×0.25 = 0.150) - The assistant provides a reasonable level of detail about how appliances operate...\n",
      "     • Consideration: 0.60 (×0.25 = 0.150) - Technical knowledge is integrated into the analysis of energy consumption and sa...\n",
      "     • Evidence: 0.60 (×0.20 = 0.120) - The assistant references specific operational patterns but does not provide mult...\n",
      "\n",
      "DETECTED INTERACTION PATTERNS\n",
      "=============================\n",
      "\n",
      "✅ Information Seeking (Confidence: 0.820)\n",
      "   Evidence: \"I want you to help me identify five behavioral changes that I make make to reduce my energy bills....\"\n",
      "   Factor Breakdown:\n",
      "     • Explicitness: 1.00 - The user explicitly asks for help in identifying behavioral ...\n",
      "     • Depth: 0.80 - The user shows thoughtful engagement by specifying their nee...\n",
      "     • Consideration: 1.00 - The user's inquiry indicates that they are actively seeking ...\n",
      "     • Evidence: 0.60 - The user provides context for their request but does not ela...\n",
      "\n",
      "✅ Constraint Articulation (Confidence: 0.820)\n",
      "   Evidence: \"the behavioral changes should not compromise my comfort and also should not bring about the adoption...\"\n",
      "   Factor Breakdown:\n",
      "     • Explicitness: 1.00 - The user clearly articulates constraints regarding comfort a...\n",
      "     • Depth: 0.80 - The user provides thoughtful reasoning about their constrain...\n",
      "     • Consideration: 1.00 - The constraints articulated by the user are central to their...\n",
      "     • Evidence: 0.60 - The user provides context about their preferences but lacks ...\n",
      "\n",
      "✅ Solution Evaluation (Confidence: 0.660)\n",
      "   Evidence: \"if I implement these suggested behaviors, how much am I likely to save....\"\n",
      "   Factor Breakdown:\n",
      "     • Explicitness: 0.80 - The user evaluates potential solutions by asking about savin...\n",
      "     • Depth: 0.60 - The user shows some understanding of the implications of the...\n",
      "     • Consideration: 0.80 - The user's inquiry about savings indicates that they are con...\n",
      "     • Evidence: 0.40 - The user references potential savings but does not provide s...\n",
      "\n",
      "✅ Commitment Expression (Confidence: 0.660)\n",
      "   Evidence: \"this is interesting. can you give me the final behavioural changes....\"\n",
      "   Factor Breakdown:\n",
      "     • Explicitness: 0.80 - The user expresses a willingness to adopt behavioral changes...\n",
      "     • Depth: 0.60 - The user shows some commitment to change but does not provid...\n",
      "     • Consideration: 0.80 - The user's focus on behavioral changes indicates a commitmen...\n",
      "     • Evidence: 0.40 - The user expresses interest in final behavioral changes but ...\n",
      "\n",
      "MULTI-FACTOR ANALYSIS SUMMARY\n",
      "============================\n",
      "User energy concepts detected: 5\n",
      "GPT energy concepts detected: 7\n",
      "Average user confidence: 0.430\n",
      "Average GPT confidence: 0.610\n",
      "Conversation focus: The user is focused on identifying behavioral changes to reduce energy bills while maintaining comfort and avoiding new technology.\n",
      "Scoring method: experimental_context_aware_multi_factor\n",
      "\n",
      "SCORING FRAMEWORK WEIGHTS\n",
      "========================\n",
      "• Explicitness: 30% - How directly and clearly the concept is mentioned using appropriate terminology\n",
      "• Depth: 25% - Quality and authenticity of engagement with the concept\n",
      "• Consideration: 25% - Whether the concept was meaningfully present in the participant's thinking\n",
      "• Evidence: 20% - Quality and authenticity of supporting evidence provided\n",
      "\n",
      "💾 Results exported to: Result/SCALE/llm_analysis_results_020.json\n",
      "\n",
      "======================================================================\n",
      "✅ Experimental Context-Aware Multi-Factor LLM Analysis completed successfully!\n",
      "🧠 Model used: gpt-4o-mini\n",
      "💾 Results exported to Result folder with subject numbering\n",
      "📊 Includes experimental context-aware multi-factor confidence scoring\n",
      "🔍 Distinguishes authentic user engagement from data-enhanced assistant analysis\n",
      "🎯 Each concept scored with participant-appropriate criteria\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"🧠 Experimental Context-Aware Multi-Factor LLM-Based GPT Chat Analyzer Ready!\")\n",
    "    print(\"📊 This version distinguishes authentic user engagement from data-enhanced assistant analysis\")\n",
    "    print(\"🔑 Make sure you have a .env file with your OpenAI API key\")\n",
    "    print(\"💰 Note: Experimental context-aware LLM analysis provides systematic confidence scores\")\n",
    "    print(\"\\\\n📋 Experimental Context-Aware Scoring Framework:\")\n",
    "    print(\"   • USER: Authentic engagement (personal insights, constraints, genuine questions)\")\n",
    "    print(\"   • ASSISTANT: Data analysis quality (synthesis, domain expertise, practical guidance)\")\n",
    "    print(\"   • Explicitness (30%): Appropriate terminology for participant role\")\n",
    "    print(\"   • Depth (25%): Authentic engagement vs data analysis quality\")\n",
    "    print(\"   • Consideration (25%): Role-appropriate conversation focus\")\n",
    "    print(\"   • Evidence (20%): Personal examples vs quantitative analysis\")\n",
    "    \n",
    "    # Uncomment to test:\n",
    "    results = analyze_single_conversation_llm(\"./Data/020/EntireConversation_extracted.json\", detection_threshold=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9b365bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_all_conversations_llm(\n",
    "    data_folder: str = \"./Data\", \n",
    "    api_key: str = None, \n",
    "    analysis_model: str = \"gpt-4o-mini\", \n",
    "    detection_threshold: float = 0.4,\n",
    "    file_pattern: str = \"EntireConversation_extracted.json\",\n",
    "    max_files: int = None,\n",
    "    resume_from: str = None,\n",
    "    start_subject: int = None,\n",
    "    end_subject: int = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Batch process all conversation files using experimental context-aware LLM analysis.\n",
    "    \n",
    "    Args:\n",
    "        data_folder (str): Root folder containing subject directories\n",
    "        api_key (str): OpenAI API key (optional - will auto-load from .env file)\n",
    "        analysis_model (str): OpenAI chat model to use for analysis\n",
    "        detection_threshold (float): Confidence threshold for concept detection (0.0-1.0)\n",
    "        file_pattern (str): Name pattern of conversation files to process\n",
    "        max_files (int): Maximum number of files to process (for testing)\n",
    "        resume_from (str): Subject ID to resume from (useful for interrupted runs)\n",
    "        start_subject (int): Starting subject number (e.g., 20 to start from subject 020)\n",
    "        end_subject (int): Ending subject number (e.g., 85 to end at subject 085)\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, Any]: Batch processing results with aggregated statistics\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    import time\n",
    "    from pathlib import Path\n",
    "    \n",
    "    print(\"🚀 Starting Batch Processing of All Conversation Files\")\n",
    "    print(f\"📂 Data folder: {data_folder}\")\n",
    "    print(f\"🧠 Analysis model: {analysis_model}\")\n",
    "    print(f\"🎯 Detection threshold: {detection_threshold}\")\n",
    "    print(f\"📄 File pattern: {file_pattern}\")\n",
    "    if start_subject is not None or end_subject is not None:\n",
    "        print(f\"📊 Subject range: {start_subject or 1} to {end_subject or 'end'}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Find all conversation files\n",
    "    search_pattern = os.path.join(data_folder, \"*\", file_pattern)\n",
    "    conversation_files = glob.glob(search_pattern)\n",
    "    conversation_files.sort()  # Process in order\n",
    "    \n",
    "    if not conversation_files:\n",
    "        print(f\"❌ No conversation files found matching pattern: {search_pattern}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"📋 Found {len(conversation_files)} conversation files total\")\n",
    "    \n",
    "    # Filter by subject number range if specified\n",
    "    if start_subject is not None or end_subject is not None:\n",
    "        filtered_files = []\n",
    "        for file_path in conversation_files:\n",
    "            subject_folder = os.path.basename(os.path.dirname(file_path))\n",
    "            try:\n",
    "                # Extract numeric part from subject folder (e.g., \"016\" -> 16)\n",
    "                subject_num = int(subject_folder.lstrip('0')) if subject_folder.isdigit() else None\n",
    "                if subject_num is None:\n",
    "                    print(f\"⚠️ Skipping non-numeric subject folder: {subject_folder}\")\n",
    "                    continue\n",
    "                    \n",
    "                # Check if subject number is in range\n",
    "                in_range = True\n",
    "                if start_subject is not None and subject_num < start_subject:\n",
    "                    in_range = False\n",
    "                if end_subject is not None and subject_num > end_subject:\n",
    "                    in_range = False\n",
    "                    \n",
    "                if in_range:\n",
    "                    filtered_files.append(file_path)\n",
    "                    \n",
    "            except ValueError:\n",
    "                print(f\"⚠️ Could not parse subject number from folder: {subject_folder}\")\n",
    "                continue\n",
    "        \n",
    "        conversation_files = filtered_files\n",
    "        print(f\"🎯 Filtered to {len(conversation_files)} files in subject range {start_subject or 1}-{end_subject or 'end'}\")\n",
    "    \n",
    "    if not conversation_files:\n",
    "        print(f\"❌ No conversation files found in specified range\")\n",
    "        return None\n",
    "    \n",
    "    # Apply max_files limit for testing\n",
    "    if max_files:\n",
    "        conversation_files = conversation_files[:max_files]\n",
    "        print(f\"🔬 Processing first {len(conversation_files)} files for testing\")\n",
    "    \n",
    "    # Resume functionality\n",
    "    if resume_from:\n",
    "        start_index = 0\n",
    "        for i, file_path in enumerate(conversation_files):\n",
    "            subject_folder = os.path.basename(os.path.dirname(file_path))\n",
    "            if subject_folder == resume_from:\n",
    "                start_index = i\n",
    "                break\n",
    "        conversation_files = conversation_files[start_index:]\n",
    "        print(f\"🔄 Resuming from subject {resume_from} ({len(conversation_files)} files remaining)\")\n",
    "    \n",
    "    # Initialize batch tracking\n",
    "    batch_results = {\n",
    "        \"processing_summary\": {\n",
    "            \"total_files\": len(conversation_files),\n",
    "            \"successful_analyses\": 0,\n",
    "            \"failed_analyses\": 0,\n",
    "            \"skipped_files\": 0,\n",
    "            \"start_time\": datetime.now().isoformat(),\n",
    "            \"analysis_model\": analysis_model,\n",
    "            \"detection_threshold\": detection_threshold,\n",
    "            \"subject_range\": {\n",
    "                \"start_subject\": start_subject,\n",
    "                \"end_subject\": end_subject,\n",
    "                \"range_specified\": start_subject is not None or end_subject is not None\n",
    "            }\n",
    "        },\n",
    "        \"individual_results\": {},\n",
    "        \"failed_files\": {},\n",
    "        \"aggregated_statistics\": {},\n",
    "        \"concept_frequency\": {},\n",
    "        \"subject_summaries\": []\n",
    "    }\n",
    "    \n",
    "    # Initialize analyzer once for the batch\n",
    "    try:\n",
    "        analyzer = LLMChatAnalyzer(api_key=api_key, analysis_model=analysis_model)\n",
    "        print(f\"✅ LLM Analyzer initialized successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to initialize LLM Analyzer: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Process each conversation file\n",
    "    for file_index, file_path in enumerate(conversation_files, 1):\n",
    "        subject_folder = os.path.basename(os.path.dirname(file_path))\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"📁 Processing {file_index}/{len(conversation_files)}: Subject {subject_folder}\")\n",
    "        print(f\"📄 File: {file_path}\")\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"⚠️ File not found, skipping: {file_path}\")\n",
    "            batch_results[\"processing_summary\"][\"skipped_files\"] += 1\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Use the existing comprehensive analysis method\n",
    "            print(f\"🧠 Starting experimental context-aware analysis...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Call the individual file analysis\n",
    "            individual_results = analyzer.comprehensive_analysis(file_path, detection_threshold=detection_threshold)\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            if individual_results:\n",
    "                # Store results\n",
    "                subject_id = individual_results.get(\"subject_id\", subject_folder)\n",
    "                batch_results[\"individual_results\"][subject_id] = individual_results\n",
    "                \n",
    "                # Export individual results\n",
    "                analyzer.export_results(individual_results)\n",
    "                \n",
    "                # Track success\n",
    "                batch_results[\"processing_summary\"][\"successful_analyses\"] += 1\n",
    "                \n",
    "                # Collect summary for aggregation\n",
    "                summary_data = {\n",
    "                    \"subject_id\": subject_id,\n",
    "                    \"processing_time\": processing_time,\n",
    "                    \"user_concepts_detected\": sum(1 for concept_data in individual_results.get(\"user_concepts\", {}).values() \n",
    "                                                if concept_data.get(\"detected\", False)),\n",
    "                    \"gpt_concepts_detected\": sum(1 for concept_data in individual_results.get(\"gpt_concepts\", {}).values() \n",
    "                                               if concept_data.get(\"detected\", False)),\n",
    "                    \"total_turns\": individual_results.get(\"conversation_metadata\", {}).get(\"total_turns\", 0),\n",
    "                    \"total_words\": individual_results.get(\"conversation_metadata\", {}).get(\"total_words\", 0),\n",
    "                    \"user_prompts\": individual_results.get(\"user_engagement\", {}).get(\"total_prompts\", 0)\n",
    "                }\n",
    "                batch_results[\"subject_summaries\"].append(summary_data)\n",
    "                \n",
    "                print(f\"✅ Subject {subject_id} analyzed successfully in {processing_time:.1f}s\")\n",
    "                print(f\"   User concepts: {summary_data['user_concepts_detected']}, GPT concepts: {summary_data['gpt_concepts_detected']}\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"⚠️ Analysis returned empty results for {subject_folder}\")\n",
    "                batch_results[\"processing_summary\"][\"failed_analyses\"] += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error analyzing {subject_folder}: {str(e)}\")\n",
    "            batch_results[\"failed_files\"][subject_folder] = str(e)\n",
    "            batch_results[\"processing_summary\"][\"failed_analyses\"] += 1\n",
    "            \n",
    "            # Continue with next file instead of stopping\n",
    "            continue\n",
    "        \n",
    "        # Progress update every 10 files\n",
    "        if file_index % 10 == 0:\n",
    "            success_rate = (batch_results[\"processing_summary\"][\"successful_analyses\"] / file_index) * 100\n",
    "            print(f\"\\n📊 Progress Update: {file_index}/{len(conversation_files)} files processed\")\n",
    "            print(f\"   Success rate: {success_rate:.1f}%\")\n",
    "            print(f\"   Successful: {batch_results['processing_summary']['successful_analyses']}\")\n",
    "            print(f\"   Failed: {batch_results['processing_summary']['failed_analyses']}\")\n",
    "    \n",
    "    # Calculate final aggregated statistics\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"📊 Calculating aggregated statistics...\")\n",
    "    \n",
    "    batch_results[\"aggregated_statistics\"] = calculate_batch_statistics(batch_results)\n",
    "    batch_results[\"concept_frequency\"] = calculate_concept_frequency(batch_results)\n",
    "    batch_results[\"processing_summary\"][\"end_time\"] = datetime.now().isoformat()\n",
    "    batch_results[\"processing_summary\"][\"total_processing_time\"] = sum(s[\"processing_time\"] for s in batch_results[\"subject_summaries\"])\n",
    "    \n",
    "    # Export batch results\n",
    "    export_batch_results(batch_results)\n",
    "    \n",
    "    # Print final summary\n",
    "    print_batch_summary(batch_results)\n",
    "    \n",
    "    return batch_results\n",
    "\n",
    "\n",
    "def calculate_batch_statistics(batch_results: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Calculate aggregated statistics across all processed conversations.\"\"\"\n",
    "    \n",
    "    summaries = batch_results[\"subject_summaries\"]\n",
    "    \n",
    "    if not summaries:\n",
    "        return {}\n",
    "    \n",
    "    stats = {\n",
    "        \"conversation_metrics\": {\n",
    "            \"avg_turns\": np.mean([s[\"total_turns\"] for s in summaries]),\n",
    "            \"avg_words\": np.mean([s[\"total_words\"] for s in summaries]),\n",
    "            \"avg_user_prompts\": np.mean([s[\"user_prompts\"] for s in summaries]),\n",
    "            \"avg_processing_time\": np.mean([s[\"processing_time\"] for s in summaries]),\n",
    "            \"total_conversations\": len(summaries)\n",
    "        },\n",
    "        \"concept_detection\": {\n",
    "            \"avg_user_concepts\": np.mean([s[\"user_concepts_detected\"] for s in summaries]),\n",
    "            \"avg_gpt_concepts\": np.mean([s[\"gpt_concepts_detected\"] for s in summaries]),\n",
    "            \"max_user_concepts\": max([s[\"user_concepts_detected\"] for s in summaries]),\n",
    "            \"max_gpt_concepts\": max([s[\"gpt_concepts_detected\"] for s in summaries]),\n",
    "            \"users_with_concepts\": sum(1 for s in summaries if s[\"user_concepts_detected\"] > 0),\n",
    "            \"conversations_with_gpt_concepts\": sum(1 for s in summaries if s[\"gpt_concepts_detected\"] > 0)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "def calculate_concept_frequency(batch_results: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Calculate frequency of each concept across all conversations.\"\"\"\n",
    "    \n",
    "    concept_counts = {\n",
    "        \"user_energy_concepts\": {},\n",
    "        \"user_interaction_patterns\": {},\n",
    "        \"gpt_energy_concepts\": {},\n",
    "        \"gpt_interaction_patterns\": {}\n",
    "    }\n",
    "    \n",
    "    total_conversations = len(batch_results[\"individual_results\"])\n",
    "    \n",
    "    for subject_id, results in batch_results[\"individual_results\"].items():\n",
    "        # Count user concepts\n",
    "        for concept_name, concept_data in results.get(\"user_concepts\", {}).items():\n",
    "            if concept_data.get(\"detected\", False):\n",
    "                if concept_name.startswith(\"energy_\"):\n",
    "                    category = \"user_energy_concepts\"\n",
    "                elif concept_name.startswith(\"interaction_\"):\n",
    "                    category = \"user_interaction_patterns\"\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                if concept_name not in concept_counts[category]:\n",
    "                    concept_counts[category][concept_name] = 0\n",
    "                concept_counts[category][concept_name] += 1\n",
    "        \n",
    "        # Count GPT concepts\n",
    "        for concept_name, concept_data in results.get(\"gpt_concepts\", {}).items():\n",
    "            if concept_data.get(\"detected\", False):\n",
    "                if concept_name.startswith(\"energy_\"):\n",
    "                    category = \"gpt_energy_concepts\"\n",
    "                elif concept_name.startswith(\"interaction_\"):\n",
    "                    category = \"gpt_interaction_patterns\"\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                if concept_name not in concept_counts[category]:\n",
    "                    concept_counts[category][concept_name] = 0\n",
    "                concept_counts[category][concept_name] += 1\n",
    "    \n",
    "    # Convert to frequencies (percentages)\n",
    "    concept_frequencies = {}\n",
    "    for category, counts in concept_counts.items():\n",
    "        concept_frequencies[category] = {\n",
    "            concept: (count / total_conversations) * 100\n",
    "            for concept, count in counts.items()\n",
    "        }\n",
    "    \n",
    "    return concept_frequencies\n",
    "\n",
    "\n",
    "def export_batch_results(batch_results: Dict[str, Any]):\n",
    "    \"\"\"Export batch processing results to files.\"\"\"\n",
    "    \n",
    "    output_folder = \"Result\"\n",
    "    os.makedirs(os.path.join(output_folder, \"SCALE\"), exist_ok=True)\n",
    "    \n",
    "    # Export main batch results\n",
    "    batch_file = os.path.join(output_folder, \"SCALE\", \"batch_analysis_results.json\")\n",
    "    with open(batch_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(batch_results, f, indent=2, default=str)\n",
    "    \n",
    "    # Export summary CSV for easy analysis\n",
    "    summary_file = os.path.join(output_folder, \"SCALE\", \"batch_summary.csv\")\n",
    "    df_summary = pd.DataFrame(batch_results[\"subject_summaries\"])\n",
    "    df_summary.to_csv(summary_file, index=False)\n",
    "    \n",
    "    # Export concept frequency CSV\n",
    "    freq_file = os.path.join(output_folder, \"SCALE\", \"concept_frequencies.csv\")\n",
    "    freq_data = []\n",
    "    for category, frequencies in batch_results[\"concept_frequency\"].items():\n",
    "        for concept, freq in frequencies.items():\n",
    "            freq_data.append({\n",
    "                \"category\": category,\n",
    "                \"concept\": concept,\n",
    "                \"frequency_percent\": freq,\n",
    "                \"participant_type\": \"user\" if \"user_\" in category else \"gpt\",\n",
    "                \"concept_type\": \"energy\" if \"energy\" in category else \"interaction\"\n",
    "            })\n",
    "    \n",
    "    if freq_data:\n",
    "        df_freq = pd.DataFrame(freq_data)\n",
    "        df_freq.to_csv(freq_file, index=False)\n",
    "    \n",
    "    print(f\"💾 Batch results exported:\")\n",
    "    print(f\"   📄 Main results: {batch_file}\")\n",
    "    print(f\"   📊 Summary CSV: {summary_file}\")\n",
    "    print(f\"   📈 Frequencies CSV: {freq_file}\")\n",
    "\n",
    "\n",
    "def print_batch_summary(batch_results: Dict[str, Any]):\n",
    "    \"\"\"Print a comprehensive summary of batch processing results.\"\"\"\n",
    "    \n",
    "    processing = batch_results[\"processing_summary\"]\n",
    "    stats = batch_results.get(\"aggregated_statistics\", {})\n",
    "    subject_range = processing.get(\"subject_range\", {})\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"🎉 BATCH PROCESSING COMPLETED!\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"📊 PROCESSING SUMMARY:\")\n",
    "    print(f\"   Total files found: {processing['total_files']}\")\n",
    "    print(f\"   Successfully analyzed: {processing['successful_analyses']}\")\n",
    "    print(f\"   Failed analyses: {processing['failed_analyses']}\")\n",
    "    print(f\"   Skipped files: {processing['skipped_files']}\")\n",
    "    \n",
    "    # Show subject range if specified\n",
    "    if subject_range.get(\"range_specified\", False):\n",
    "        start = subject_range.get(\"start_subject\", \"start\")\n",
    "        end = subject_range.get(\"end_subject\", \"end\")\n",
    "        print(f\"   Subject range processed: {start} to {end}\")\n",
    "    \n",
    "    if processing['successful_analyses'] > 0:\n",
    "        success_rate = (processing['successful_analyses'] / processing['total_files']) * 100\n",
    "        print(f\"   Success rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    if stats:\n",
    "        conv_metrics = stats.get(\"conversation_metrics\", {})\n",
    "        concept_metrics = stats.get(\"concept_detection\", {})\n",
    "        \n",
    "        print(f\"\\n📈 CONVERSATION STATISTICS:\")\n",
    "        print(f\"   Average turns per conversation: {conv_metrics.get('avg_turns', 0):.1f}\")\n",
    "        print(f\"   Average words per conversation: {conv_metrics.get('avg_words', 0):.0f}\")\n",
    "        print(f\"   Average user prompts: {conv_metrics.get('avg_user_prompts', 0):.1f}\")\n",
    "        print(f\"   Average processing time: {conv_metrics.get('avg_processing_time', 0):.1f}s\")\n",
    "        \n",
    "        print(f\"\\n🎯 CONCEPT DETECTION STATISTICS:\")\n",
    "        print(f\"   Average user concepts detected: {concept_metrics.get('avg_user_concepts', 0):.1f}\")\n",
    "        print(f\"   Average GPT concepts detected: {concept_metrics.get('avg_gpt_concepts', 0):.1f}\")\n",
    "        print(f\"   Users with detected concepts: {concept_metrics.get('users_with_concepts', 0)}/{conv_metrics.get('total_conversations', 0)}\")\n",
    "        print(f\"   Conversations with GPT concepts: {concept_metrics.get('conversations_with_gpt_concepts', 0)}/{conv_metrics.get('total_conversations', 0)}\")\n",
    "    \n",
    "    if batch_results.get(\"failed_files\"):\n",
    "        print(f\"\\n⚠️ FAILED FILES:\")\n",
    "        for subject, error in batch_results[\"failed_files\"].items():\n",
    "            print(f\"   {subject}: {error}\")\n",
    "    \n",
    "    print(f\"\\n💾 Results exported to Result/SCALE/ folder\")\n",
    "    print(f\"🔍 Individual analysis reports available for each subject\")\n",
    "    print(f\"📊 Aggregated statistics available in CSV format\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "\n",
    "# Convenience function for quick batch processing\n",
    "def run_batch_analysis(\n",
    "    data_folder: str = \"./Data\",\n",
    "    detection_threshold: float = 0.4,\n",
    "    analysis_model: str = \"gpt-4o-mini\",\n",
    "    max_files: int = None,\n",
    "    start_subject: int = None,\n",
    "    end_subject: int = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Quick function to run batch analysis with default settings.\n",
    "    \n",
    "    Args:\n",
    "        data_folder (str): Path to data folder containing subject directories\n",
    "        detection_threshold (float): Confidence threshold for concept detection\n",
    "        analysis_model (str): OpenAI model to use\n",
    "        max_files (int): Limit number of files for testing (None = process all)\n",
    "        start_subject (int): Starting subject number (e.g., 20 to start from subject 020)\n",
    "        end_subject (int): Ending subject number (e.g., 85 to end at subject 085)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🚀 Starting Quick Batch Analysis\")\n",
    "    print(f\"📂 Data folder: {data_folder}\")\n",
    "    print(f\"🎯 Detection threshold: {detection_threshold}\")\n",
    "    print(f\"🧠 Analysis model: {analysis_model}\")\n",
    "    \n",
    "    if max_files:\n",
    "        print(f\"🔬 Testing mode: Processing only {max_files} files\")\n",
    "    \n",
    "    if start_subject is not None or end_subject is not None:\n",
    "        print(f\"📊 Subject range: {start_subject or 1} to {end_subject or 'end'}\")\n",
    "    \n",
    "    results = analyze_all_conversations_llm(\n",
    "        data_folder=data_folder,\n",
    "        detection_threshold=detection_threshold,\n",
    "        analysis_model=analysis_model,\n",
    "        max_files=max_files,\n",
    "        start_subject=start_subject,\n",
    "        end_subject=end_subject\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def process_subject_range(\n",
    "    start_subject: int,\n",
    "    end_subject: int,\n",
    "    data_folder: str = \"./Data\",\n",
    "    detection_threshold: float = 0.4,\n",
    "    analysis_model: str = \"gpt-4o-mini\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Process a specific range of subjects (e.g., subjects 20-85).\n",
    "    \n",
    "    Args:\n",
    "        start_subject (int): Starting subject number (e.g., 20 for subject 020)\n",
    "        end_subject (int): Ending subject number (e.g., 85 for subject 085)\n",
    "        data_folder (str): Path to data folder containing subject directories\n",
    "        detection_threshold (float): Confidence threshold for concept detection\n",
    "        analysis_model (str): OpenAI model to use\n",
    "        \n",
    "    Example:\n",
    "        # Process subjects 20 through 85\n",
    "        results = process_subject_range(20, 85)\n",
    "        \n",
    "        # Process subjects 1 through 30\n",
    "        results = process_subject_range(1, 30)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"🎯 Processing Subject Range: {start_subject} to {end_subject}\")\n",
    "    print(f\"📂 Looking for folders like: {start_subject:03d}, {start_subject+1:03d}, ..., {end_subject:03d}\")\n",
    "    \n",
    "    results = analyze_all_conversations_llm(\n",
    "        data_folder=data_folder,\n",
    "        detection_threshold=detection_threshold,\n",
    "        analysis_model=analysis_model,\n",
    "        start_subject=start_subject,\n",
    "        end_subject=end_subject\n",
    "    )\n",
    "    \n",
    "    if results:\n",
    "        processed_subjects = len(results[\"subject_summaries\"])\n",
    "        print(f\"\\n✅ Successfully processed {processed_subjects} subjects in range {start_subject}-{end_subject}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "418cf076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Quick Batch Analysis\n",
      "📂 Data folder: ./Data\n",
      "🎯 Detection threshold: 0.4\n",
      "🧠 Analysis model: gpt-4o-mini\n",
      "📊 Subject range: 66 to 85\n",
      "🚀 Starting Batch Processing of All Conversation Files\n",
      "📂 Data folder: ./Data\n",
      "🧠 Analysis model: gpt-4o-mini\n",
      "🎯 Detection threshold: 0.4\n",
      "📄 File pattern: EntireConversation_extracted.json\n",
      "📊 Subject range: 66 to 85\n",
      "======================================================================\n",
      "📋 Found 85 conversation files total\n",
      "🎯 Filtered to 20 files in subject range 66-85\n",
      "🧠 Initialized with OpenAI chat model: gpt-4o-mini\n",
      "🔑 API key loaded successfully from parameter\n",
      "📊 Using experimental context-aware multi-factor confidence scoring framework\n",
      "✅ LLM Analyzer initialized successfully\n",
      "\n",
      "==================================================\n",
      "📁 Processing 1/20: Subject 066\n",
      "📄 File: ./Data/066/EntireConversation_extracted.json\n",
      "🧠 Starting experimental context-aware analysis...\n",
      "📖 Loading conversation from: ./Data/066/EntireConversation_extracted.json\n",
      "🧠 Analyzing user concepts (authentic engagement focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🧠 Analyzing GPT concepts (data analysis quality focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🔍 Applying detection threshold to user analysis...\n",
      "🔍 Applying detection threshold to GPT analysis...\n",
      "📊 Analyzing engagement patterns...\n",
      "💾 Results exported to: Result/SCALE/llm_analysis_results_066.json\n",
      "✅ Subject 066 analyzed successfully in 100.3s\n",
      "   User concepts: 4, GPT concepts: 9\n",
      "\n",
      "==================================================\n",
      "📁 Processing 2/20: Subject 067\n",
      "📄 File: ./Data/067/EntireConversation_extracted.json\n",
      "🧠 Starting experimental context-aware analysis...\n",
      "📖 Loading conversation from: ./Data/067/EntireConversation_extracted.json\n",
      "🧠 Analyzing user concepts (authentic engagement focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🧠 Analyzing GPT concepts (data analysis quality focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🔍 Applying detection threshold to user analysis...\n",
      "🔍 Applying detection threshold to GPT analysis...\n",
      "📊 Analyzing engagement patterns...\n",
      "💾 Results exported to: Result/SCALE/llm_analysis_results_067.json\n",
      "✅ Subject 067 analyzed successfully in 106.3s\n",
      "   User concepts: 3, GPT concepts: 6\n",
      "\n",
      "==================================================\n",
      "📁 Processing 3/20: Subject 068\n",
      "📄 File: ./Data/068/EntireConversation_extracted.json\n",
      "🧠 Starting experimental context-aware analysis...\n",
      "📖 Loading conversation from: ./Data/068/EntireConversation_extracted.json\n",
      "🧠 Analyzing user concepts (authentic engagement focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🧠 Analyzing GPT concepts (data analysis quality focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "⚠️ API error (attempt 1): float() argument must be a string or a real number, not 'dict'\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🔍 Applying detection threshold to user analysis...\n",
      "🔍 Applying detection threshold to GPT analysis...\n",
      "📊 Analyzing engagement patterns...\n",
      "💾 Results exported to: Result/SCALE/llm_analysis_results_068.json\n",
      "✅ Subject 068 analyzed successfully in 171.3s\n",
      "   User concepts: 8, GPT concepts: 8\n",
      "\n",
      "==================================================\n",
      "📁 Processing 4/20: Subject 069\n",
      "📄 File: ./Data/069/EntireConversation_extracted.json\n",
      "🧠 Starting experimental context-aware analysis...\n",
      "📖 Loading conversation from: ./Data/069/EntireConversation_extracted.json\n",
      "🧠 Analyzing user concepts (authentic engagement focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "⚠️ JSON parsing error (attempt 1): Expecting ',' delimiter: line 4883 column 3 (char 46098)\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🧠 Analyzing GPT concepts (data analysis quality focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🔍 Applying detection threshold to user analysis...\n",
      "🔍 Applying detection threshold to GPT analysis...\n",
      "📊 Analyzing engagement patterns...\n",
      "💾 Results exported to: Result/SCALE/llm_analysis_results_069.json\n",
      "✅ Subject 069 analyzed successfully in 398.3s\n",
      "   User concepts: 5, GPT concepts: 7\n",
      "\n",
      "==================================================\n",
      "📁 Processing 5/20: Subject 070\n",
      "📄 File: ./Data/070/EntireConversation_extracted.json\n",
      "🧠 Starting experimental context-aware analysis...\n",
      "📖 Loading conversation from: ./Data/070/EntireConversation_extracted.json\n",
      "🧠 Analyzing user concepts (authentic engagement focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "⚠️ API error (attempt 1): unsupported operand type(s) for *: 'dict' and 'float'\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🧠 Analyzing GPT concepts (data analysis quality focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "⚠️ API error (attempt 1): float() argument must be a string or a real number, not 'dict'\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🔍 Applying detection threshold to user analysis...\n",
      "🔍 Applying detection threshold to GPT analysis...\n",
      "📊 Analyzing engagement patterns...\n",
      "💾 Results exported to: Result/SCALE/llm_analysis_results_070.json\n",
      "✅ Subject 070 analyzed successfully in 198.7s\n",
      "   User concepts: 5, GPT concepts: 10\n",
      "\n",
      "==================================================\n",
      "📁 Processing 6/20: Subject 071\n",
      "📄 File: ./Data/071/EntireConversation_extracted.json\n",
      "🧠 Starting experimental context-aware analysis...\n",
      "📖 Loading conversation from: ./Data/071/EntireConversation_extracted.json\n",
      "🧠 Analyzing user concepts (authentic engagement focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🧠 Analyzing GPT concepts (data analysis quality focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🔍 Applying detection threshold to user analysis...\n",
      "🔍 Applying detection threshold to GPT analysis...\n",
      "📊 Analyzing engagement patterns...\n",
      "💾 Results exported to: Result/SCALE/llm_analysis_results_071.json\n",
      "✅ Subject 071 analyzed successfully in 126.7s\n",
      "   User concepts: 3, GPT concepts: 7\n",
      "\n",
      "==================================================\n",
      "📁 Processing 7/20: Subject 072\n",
      "📄 File: ./Data/072/EntireConversation_extracted.json\n",
      "🧠 Starting experimental context-aware analysis...\n",
      "📖 Loading conversation from: ./Data/072/EntireConversation_extracted.json\n",
      "🧠 Analyzing user concepts (authentic engagement focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🧠 Analyzing GPT concepts (data analysis quality focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🔍 Applying detection threshold to user analysis...\n",
      "🔍 Applying detection threshold to GPT analysis...\n",
      "📊 Analyzing engagement patterns...\n",
      "💾 Results exported to: Result/SCALE/llm_analysis_results_072.json\n",
      "✅ Subject 072 analyzed successfully in 96.9s\n",
      "   User concepts: 5, GPT concepts: 8\n",
      "\n",
      "==================================================\n",
      "📁 Processing 8/20: Subject 073\n",
      "📄 File: ./Data/073/EntireConversation_extracted.json\n",
      "🧠 Starting experimental context-aware analysis...\n",
      "📖 Loading conversation from: ./Data/073/EntireConversation_extracted.json\n",
      "🧠 Analyzing user concepts (authentic engagement focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🧠 Analyzing GPT concepts (data analysis quality focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🔍 Applying detection threshold to user analysis...\n",
      "🔍 Applying detection threshold to GPT analysis...\n",
      "📊 Analyzing engagement patterns...\n",
      "💾 Results exported to: Result/SCALE/llm_analysis_results_073.json\n",
      "✅ Subject 073 analyzed successfully in 108.5s\n",
      "   User concepts: 5, GPT concepts: 11\n",
      "\n",
      "==================================================\n",
      "📁 Processing 9/20: Subject 074\n",
      "📄 File: ./Data/074/EntireConversation_extracted.json\n",
      "🧠 Starting experimental context-aware analysis...\n",
      "📖 Loading conversation from: ./Data/074/EntireConversation_extracted.json\n",
      "🧠 Analyzing user concepts (authentic engagement focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "⚠️ JSON parsing error (attempt 1): Expecting ',' delimiter: line 4912 column 3 (char 45597)\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🧠 Analyzing GPT concepts (data analysis quality focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🔍 Applying detection threshold to user analysis...\n",
      "🔍 Applying detection threshold to GPT analysis...\n",
      "📊 Analyzing engagement patterns...\n",
      "💾 Results exported to: Result/SCALE/llm_analysis_results_074.json\n",
      "✅ Subject 074 analyzed successfully in 448.1s\n",
      "   User concepts: 2, GPT concepts: 6\n",
      "\n",
      "==================================================\n",
      "📁 Processing 10/20: Subject 075\n",
      "📄 File: ./Data/075/EntireConversation_extracted.json\n",
      "🧠 Starting experimental context-aware analysis...\n",
      "📖 Loading conversation from: ./Data/075/EntireConversation_extracted.json\n",
      "🧠 Analyzing user concepts (authentic engagement focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "⚠️ JSON parsing error (attempt 1): Expecting ',' delimiter: line 266 column 13956 (char 26886)\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "⚠️ JSON parsing error (attempt 2): Expecting ',' delimiter: line 4929 column 1 (char 45316)\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🧠 Analyzing GPT concepts (data analysis quality focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🔍 Applying detection threshold to user analysis...\n",
      "🔍 Applying detection threshold to GPT analysis...\n",
      "📊 Analyzing engagement patterns...\n",
      "💾 Results exported to: Result/SCALE/llm_analysis_results_075.json\n",
      "✅ Subject 075 analyzed successfully in 922.8s\n",
      "   User concepts: 5, GPT concepts: 9\n",
      "\n",
      "📊 Progress Update: 10/20 files processed\n",
      "   Success rate: 100.0%\n",
      "   Successful: 10\n",
      "   Failed: 0\n",
      "\n",
      "==================================================\n",
      "📁 Processing 11/20: Subject 076\n",
      "📄 File: ./Data/076/EntireConversation_extracted.json\n",
      "🧠 Starting experimental context-aware analysis...\n",
      "📖 Loading conversation from: ./Data/076/EntireConversation_extracted.json\n",
      "🧠 Analyzing user concepts (authentic engagement focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🧠 Analyzing GPT concepts (data analysis quality focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🔍 Applying detection threshold to user analysis...\n",
      "🔍 Applying detection threshold to GPT analysis...\n",
      "📊 Analyzing engagement patterns...\n",
      "💾 Results exported to: Result/SCALE/llm_analysis_results_076.json\n",
      "✅ Subject 076 analyzed successfully in 132.0s\n",
      "   User concepts: 9, GPT concepts: 10\n",
      "\n",
      "==================================================\n",
      "📁 Processing 12/20: Subject 077\n",
      "📄 File: ./Data/077/EntireConversation_extracted.json\n",
      "🧠 Starting experimental context-aware analysis...\n",
      "📖 Loading conversation from: ./Data/077/EntireConversation_extracted.json\n",
      "🧠 Analyzing user concepts (authentic engagement focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🧠 Analyzing GPT concepts (data analysis quality focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🔍 Applying detection threshold to user analysis...\n",
      "🔍 Applying detection threshold to GPT analysis...\n",
      "📊 Analyzing engagement patterns...\n",
      "💾 Results exported to: Result/SCALE/llm_analysis_results_077.json\n",
      "✅ Subject 077 analyzed successfully in 70.1s\n",
      "   User concepts: 3, GPT concepts: 7\n",
      "\n",
      "==================================================\n",
      "📁 Processing 13/20: Subject 078\n",
      "📄 File: ./Data/078/EntireConversation_extracted.json\n",
      "🧠 Starting experimental context-aware analysis...\n",
      "📖 Loading conversation from: ./Data/078/EntireConversation_extracted.json\n",
      "🧠 Analyzing user concepts (authentic engagement focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🧠 Analyzing GPT concepts (data analysis quality focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🔍 Applying detection threshold to user analysis...\n",
      "🔍 Applying detection threshold to GPT analysis...\n",
      "📊 Analyzing engagement patterns...\n",
      "💾 Results exported to: Result/SCALE/llm_analysis_results_078.json\n",
      "✅ Subject 078 analyzed successfully in 67.8s\n",
      "   User concepts: 3, GPT concepts: 7\n",
      "\n",
      "==================================================\n",
      "📁 Processing 14/20: Subject 079\n",
      "📄 File: ./Data/079/EntireConversation_extracted.json\n",
      "🧠 Starting experimental context-aware analysis...\n",
      "📖 Loading conversation from: ./Data/079/EntireConversation_extracted.json\n",
      "🧠 Analyzing user concepts (authentic engagement focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🧠 Analyzing GPT concepts (data analysis quality focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🔍 Applying detection threshold to user analysis...\n",
      "🔍 Applying detection threshold to GPT analysis...\n",
      "📊 Analyzing engagement patterns...\n",
      "💾 Results exported to: Result/SCALE/llm_analysis_results_079.json\n",
      "✅ Subject 079 analyzed successfully in 69.2s\n",
      "   User concepts: 3, GPT concepts: 6\n",
      "\n",
      "==================================================\n",
      "📁 Processing 15/20: Subject 080\n",
      "📄 File: ./Data/080/EntireConversation_extracted.json\n",
      "🧠 Starting experimental context-aware analysis...\n",
      "📖 Loading conversation from: ./Data/080/EntireConversation_extracted.json\n",
      "🧠 Analyzing user concepts (authentic engagement focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🧠 Analyzing GPT concepts (data analysis quality focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🔍 Applying detection threshold to user analysis...\n",
      "🔍 Applying detection threshold to GPT analysis...\n",
      "📊 Analyzing engagement patterns...\n",
      "💾 Results exported to: Result/SCALE/llm_analysis_results_080.json\n",
      "✅ Subject 080 analyzed successfully in 68.4s\n",
      "   User concepts: 4, GPT concepts: 6\n",
      "\n",
      "==================================================\n",
      "📁 Processing 16/20: Subject 081\n",
      "📄 File: ./Data/081/EntireConversation_extracted.json\n",
      "🧠 Starting experimental context-aware analysis...\n",
      "📖 Loading conversation from: ./Data/081/EntireConversation_extracted.json\n",
      "🧠 Analyzing user concepts (authentic engagement focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🧠 Analyzing GPT concepts (data analysis quality focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🔍 Applying detection threshold to user analysis...\n",
      "🔍 Applying detection threshold to GPT analysis...\n",
      "📊 Analyzing engagement patterns...\n",
      "💾 Results exported to: Result/SCALE/llm_analysis_results_081.json\n",
      "✅ Subject 081 analyzed successfully in 72.2s\n",
      "   User concepts: 5, GPT concepts: 6\n",
      "\n",
      "==================================================\n",
      "📁 Processing 17/20: Subject 082\n",
      "📄 File: ./Data/082/EntireConversation_extracted.json\n",
      "🧠 Starting experimental context-aware analysis...\n",
      "📖 Loading conversation from: ./Data/082/EntireConversation_extracted.json\n",
      "🧠 Analyzing user concepts (authentic engagement focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🧠 Analyzing GPT concepts (data analysis quality focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "⚠️ API error (attempt 1): float() argument must be a string or a real number, not 'dict'\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🔍 Applying detection threshold to user analysis...\n",
      "🔍 Applying detection threshold to GPT analysis...\n",
      "📊 Analyzing engagement patterns...\n",
      "💾 Results exported to: Result/SCALE/llm_analysis_results_082.json\n",
      "✅ Subject 082 analyzed successfully in 120.6s\n",
      "   User concepts: 5, GPT concepts: 6\n",
      "\n",
      "==================================================\n",
      "📁 Processing 18/20: Subject 083\n",
      "📄 File: ./Data/083/EntireConversation_extracted.json\n",
      "🧠 Starting experimental context-aware analysis...\n",
      "📖 Loading conversation from: ./Data/083/EntireConversation_extracted.json\n",
      "🧠 Analyzing user concepts (authentic engagement focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🧠 Analyzing GPT concepts (data analysis quality focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🔍 Applying detection threshold to user analysis...\n",
      "🔍 Applying detection threshold to GPT analysis...\n",
      "📊 Analyzing engagement patterns...\n",
      "💾 Results exported to: Result/SCALE/llm_analysis_results_083.json\n",
      "✅ Subject 083 analyzed successfully in 74.9s\n",
      "   User concepts: 2, GPT concepts: 8\n",
      "\n",
      "==================================================\n",
      "📁 Processing 19/20: Subject 084\n",
      "📄 File: ./Data/084/EntireConversation_extracted.json\n",
      "🧠 Starting experimental context-aware analysis...\n",
      "📖 Loading conversation from: ./Data/084/EntireConversation_extracted.json\n",
      "🧠 Analyzing user concepts (authentic engagement focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🧠 Analyzing GPT concepts (data analysis quality focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🔍 Applying detection threshold to user analysis...\n",
      "🔍 Applying detection threshold to GPT analysis...\n",
      "📊 Analyzing engagement patterns...\n",
      "💾 Results exported to: Result/SCALE/llm_analysis_results_084.json\n",
      "✅ Subject 084 analyzed successfully in 73.0s\n",
      "   User concepts: 6, GPT concepts: 8\n",
      "\n",
      "==================================================\n",
      "📁 Processing 20/20: Subject 085\n",
      "📄 File: ./Data/085/EntireConversation_extracted.json\n",
      "🧠 Starting experimental context-aware analysis...\n",
      "📖 Loading conversation from: ./Data/085/EntireConversation_extracted.json\n",
      "🧠 Analyzing user concepts (authentic engagement focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🧠 Analyzing GPT concepts (data analysis quality focus)...\n",
      "🧠 Analyzing concepts with gpt-4o-mini (experimental context-aware)...\n",
      "🔍 Applying detection threshold to user analysis...\n",
      "🔍 Applying detection threshold to GPT analysis...\n",
      "📊 Analyzing engagement patterns...\n",
      "💾 Results exported to: Result/SCALE/llm_analysis_results_085.json\n",
      "✅ Subject 085 analyzed successfully in 62.9s\n",
      "   User concepts: 7, GPT concepts: 9\n",
      "\n",
      "📊 Progress Update: 20/20 files processed\n",
      "   Success rate: 100.0%\n",
      "   Successful: 20\n",
      "   Failed: 0\n",
      "\n",
      "======================================================================\n",
      "📊 Calculating aggregated statistics...\n",
      "💾 Batch results exported:\n",
      "   📄 Main results: Result/SCALE/batch_analysis_results.json\n",
      "   📊 Summary CSV: Result/SCALE/batch_summary.csv\n",
      "   📈 Frequencies CSV: Result/SCALE/concept_frequencies.csv\n",
      "\n",
      "======================================================================\n",
      "🎉 BATCH PROCESSING COMPLETED!\n",
      "======================================================================\n",
      "📊 PROCESSING SUMMARY:\n",
      "   Total files found: 20\n",
      "   Successfully analyzed: 20\n",
      "   Failed analyses: 0\n",
      "   Skipped files: 0\n",
      "   Subject range processed: 66 to 85\n",
      "   Success rate: 100.0%\n",
      "\n",
      "📈 CONVERSATION STATISTICS:\n",
      "   Average turns per conversation: 6.5\n",
      "   Average words per conversation: 611\n",
      "   Average user prompts: 3.2\n",
      "   Average processing time: 174.4s\n",
      "\n",
      "🎯 CONCEPT DETECTION STATISTICS:\n",
      "   Average user concepts detected: 4.6\n",
      "   Average GPT concepts detected: 7.7\n",
      "   Users with detected concepts: 20/20\n",
      "   Conversations with GPT concepts: 20/20\n",
      "\n",
      "💾 Results exported to Result/SCALE/ folder\n",
      "🔍 Individual analysis reports available for each subject\n",
      "📊 Aggregated statistics available in CSV format\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "results = run_batch_analysis(\n",
    "    data_folder=\"./Data\",\n",
    "    start_subject=66,\n",
    "    end_subject=85,\n",
    "    detection_threshold=0.4\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
